{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n",
      "2023-08-02 11:52:10.493502: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from pyscf import gto, scf, md\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os \n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('H', [0.7, 0.0, 0.0]), ('H', [-0.7, 0.0, 0.0])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2 = gto.Mole()\n",
    "h2.atom = [['H', (0.7, 0, 0)], ['H', (-0.7, 0, 0)]]\n",
    "h2.basis = 'ccpvdz'\n",
    "h2.unit = 'B'\n",
    "h2.build() \n",
    "h2._atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -1.12870944897989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.1287094489798908"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf = scf.RHF(h2)\n",
    "mf.kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycas = mf.CASSCF(2, 2)\n",
    "myscanner = mycas.nuc_grad_method().as_scanner()\n",
    "\n",
    "# Generate the integrator\n",
    "# sets the time step to 5 a.u. and will run for 100 steps\n",
    "# or for 50 a.u.\n",
    "myintegrator = md.NVE(myscanner,\n",
    "                            dt=1,\n",
    "                            steps=1200,\n",
    "                            energy_output=\"BOMD.md.energies\",\n",
    "                            trajectory_output=\"BOMD.md.xyz\",\n",
    "                            verbose=0).run()\n",
    "\n",
    "# Note that we can also just pass the CASSCF object directly to\n",
    "# generate the integrator and it will automatically convert it to a scanner\n",
    "# myintegrator = pyscf.md.NVE(mycas, dt=5, steps=100)\n",
    "\n",
    "# Close the file streams for the energy and trajectory.\n",
    "myintegrator.energy_output.close()\n",
    "myintegrator.trajectory_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = 30\n",
    "coords = [[x, y, 0] for x in range(ticks) for y in range(ticks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "raw_data = open(\"../../data/BOMD.md.xyz\", \"r\").read() + \"2\\n\"\n",
    "raw_data = raw_data.split(\"MD Time\")[1:]\n",
    "R = []\n",
    "densities = []\n",
    "corrs = []\n",
    "data = {}\n",
    "#for n in range(len(raw_data)):\n",
    "for n in [0]:\n",
    "    print(n)\n",
    "    data_point = raw_data[n]\n",
    "    data_point = data_point.split(\"\\n\")[1:-2]\n",
    "    data_point = [re.sub(r'\\s+', ',', coords).split(\",\") for coords in data_point]\n",
    "    data_point = [[atom, (x, y, z)] for [atom, x, y, z] in data_point]\n",
    "    mol = gto.M()\n",
    "    mol.atom = data_point\n",
    "    mol.basis = \"ccpvdz\"\n",
    "    mol.build()\n",
    "    hf = scf.RHF(mol).run(verbose=0)\n",
    "    rdm1 = hf.make_rdm1()\n",
    "    rdm2 = hf.make_rdm2()\n",
    "    ao_vals = np.array(mol.eval_ao(\"GTOval_sph\", coords))\n",
    "    n_coords = len(coords)\n",
    "    n_orbitals = ao_vals.shape[-1]\n",
    "    #ao_vals_pointwise = [[coords[i].tolist(), ao_vals_pointwise[i].tolist()] for i in range(n_coords)]\n",
    "    density_for_point = []\n",
    "    corr_for_point = []\n",
    "    for i in range(n_coords):\n",
    "        [x, y, _] = coords[i]\n",
    "        ao_vals_for_point = ao_vals[i]\n",
    "        ao_vals_for_point = ao_vals_for_point.flatten()\n",
    "        z = np.einsum(\"ij,i,j->\", rdm1, ao_vals_for_point, ao_vals_for_point)\n",
    "        density_for_point.append(z)\n",
    "        for j in range(n_coords):\n",
    "            [x1, y1, _] = coords[j]\n",
    "            ao_vals_for_point_1 = ao_vals[j]\n",
    "            ao_vals_for_point_1 = ao_vals_for_point_1.flatten()\n",
    "            rho_corr = np.einsum(\"iijj,i,i,j,j->\", rdm2, ao_vals_for_point_1, ao_vals_for_point_1, ao_vals_for_point_1, ao_vals_for_point_1)\n",
    "            corr_for_point.append(rho_corr)\n",
    "    R.append([[x, y, z] for [atom, (x, y, z)] in data_point])\n",
    "    densities.append(density_for_point)\n",
    "    corrs.append(corr_for_point)\n",
    "    for key in [\"R\", \"densities\", \"corrs\"]:\n",
    "        data[key] = eval(key)\n",
    "    count = str(n)\n",
    "    filename = f\"md_h2_{str(n+1)}.npz\"\n",
    "    with open(f\"md_h2/{filename}\", \"wb\") as file:\n",
    "        pickle.dump(data, file)\n",
    "    R = []\n",
    "    densities = []\n",
    "    corrs = []\n",
    "    data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"R\", \"densities\", \"corrs\"]\n",
    "data_dict = {key: None for key in keys}\n",
    "filenames = os.listdir(\"./md_h2\")\n",
    "filenames.sort()\n",
    "for f in filenames:\n",
    "    file = np.load(f\"./md_h2/{f}\", allow_pickle=True)\n",
    "    f = np.load(\"./md_h2/md_h2_1.npz\", allow_pickle=True)\n",
    "    for key in keys:\n",
    "        if data_dict[key] is not None:\n",
    "            data_dict[key] = np.concatenate((data_dict[key].astype(np.float32), np.array(f[key]).astype(np.float32)), axis=0)\n",
    "        else:\n",
    "            data_dict[key] = np.array(f[key])\n",
    "total_filename = \"md_h2_1_61.npz\"\n",
    "if total_filename not in filenames:\n",
    "    with open(f\"./md_h2/{total_filename}\", \"wb\") as file:\n",
    "        pickle.dump(data_dict, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create coords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_file = np.load(f\"./md_h2/{total_filename}\", allow_pickle=True)\n",
    "R = total_file[\"R\"]\n",
    "R_mean = R.mean(axis=0)\n",
    "center = R_mean.mean(axis=0)\n",
    "R_reshaped = R.reshape((61 * 2, 3))\n",
    "max_abs_value = np.sqrt((R_reshaped * R_reshaped).sum(axis=-1)).max()\n",
    "ticks = 30\n",
    "array = np.linspace(-max_abs_value / 2, max_abs_value, ticks)\n",
    "coords = np.array([[x, y, 0] for x in array for y in array])\n",
    "total_file[\"coords\"] = coords\n",
    "pickle.dump(total_file, open(f\"./md_h2/{total_filename}\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_file = np.load(f\"./md_h2/{total_filename}\", allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
