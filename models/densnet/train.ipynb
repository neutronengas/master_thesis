{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39myaml\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdensnet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdensnet\u001b[39;00m \u001b[39mimport\u001b[39;00m DensNet\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from densnet.model.densnet import DensNet\n",
    "from densnet.model.activations import swish\n",
    "from densnet.training.metrics import Metrics\n",
    "from densnet.training.trainer import Trainer\n",
    "from densnet.training.data_container import DataContainer\n",
    "from densnet.training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logger\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "num_basis_fct = config['num_basis_fct']\n",
    "emb_size = config['emb_size']\n",
    "num_interaction_blocks = config['num_interaction_blocks']\n",
    "ao_vals = config['ao_vals']\n",
    "num_grid_points = config['num_grid_points']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "target = config['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create directories***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: ./logging/20230805_165515_5iTVhDCR_md_h2.npz_densities_final\n"
     ]
    }
   ],
   "source": [
    "# Used for creating a random \"unique\" id for this run\n",
    "def id_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# Create directories\n",
    "# A unique directory name is created for this run based on the input\n",
    "if restart is None:\n",
    "    directory = (logdir + \"/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + id_generator()\n",
    "                 + \"_\" + os.path.basename(dataset)\n",
    "                 + \"_\" + '-'.join(target)\n",
    "                 + \"_\" + comment)\n",
    "else:\n",
    "    directory = restart\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "best_dir = os.path.join(directory, 'best')\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, 'logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "best_loss_file = os.path.join(best_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(best_dir, 'ckpt')\n",
    "step_ckpt_folder = log_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create summary writer and metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "train = {}\n",
    "validation = {}\n",
    "train['metrics'] = Metrics('train', target)\n",
    "validation['metrics'] = Metrics('val', target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_container = DataContainer(dataset, target, 0.2)\n",
    "\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size, seed=data_seed, randomized=True)\n",
    "\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DensNet(ao_vals=ao_vals, num_interaction_blocks=num_interaction_blocks, num_grid_points=num_grid_points, activation=swish)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save/load best recorded loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/Documents/MA/models/densnet/densnet/training/metrics.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.mean(np.log(self.maes)).item()\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(best_loss_file):\n",
    "    loss_file = np.load(best_loss_file)\n",
    "    metrics_best = {k: v.item() for k, v in loss_file.items()}\n",
    "else:\n",
    "    metrics_best = validation['metrics'].result()\n",
    "    for key in metrics_best.keys():\n",
    "        metrics_best[key] = np.inf\n",
    "    metrics_best['step'] = 0\n",
    "    np.savez(best_loss_file, **metrics_best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize trainer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps, decay_steps, decay_rate, ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set up checkpointing and load latest checkpoint*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n",
      "Tensor(\"densnet/output/matact/mul_1:0\", shape=(None, 900), dtype=float32)\n",
      "asd\n",
      "Tensor(\"densnet/output/matact/mul_1:0\", shape=(None, 900), dtype=float32)\n",
      "asd\n",
      "100/10000 (epoch 51):Loss: train=0.181179, val=0.160141;logMAE: train=-1.708271, val=-1.831703\n",
      "200/10000 (epoch 101):Loss: train=0.173064, val=0.159250;logMAE: train=-1.754095, val=-1.837278\n",
      "300/10000 (epoch 151):Loss: train=0.155813, val=0.156876;logMAE: train=-1.859102, val=-1.852302\n",
      "400/10000 (epoch 201):Loss: train=0.126941, val=0.152256;logMAE: train=-2.064033, val=-1.882189\n",
      "500/10000 (epoch 251):Loss: train=0.083818, val=0.144591;logMAE: train=-2.479111, val=-1.933845\n",
      "600/10000 (epoch 301):Loss: train=0.051658, val=0.134337;logMAE: train=-2.963102, val=-2.007405\n",
      "700/10000 (epoch 351):Loss: train=0.045154, val=0.123705;logMAE: train=-3.097672, val=-2.089856\n",
      "800/10000 (epoch 401):Loss: train=0.043486, val=0.113813;logMAE: train=-3.135321, val=-2.173197\n",
      "900/10000 (epoch 451):Loss: train=0.042412, val=0.104666;logMAE: train=-3.160330, val=-2.256979\n",
      "1000/10000 (epoch 501):Loss: train=0.041523, val=0.096199;logMAE: train=-3.181508, val=-2.341335\n",
      "1100/10000 (epoch 551):Loss: train=0.040710, val=0.088369;logMAE: train=-3.201282, val=-2.426236\n",
      "1200/10000 (epoch 601):Loss: train=0.039930, val=0.081222;logMAE: train=-3.220622, val=-2.510566\n",
      "1300/10000 (epoch 651):Loss: train=0.039114, val=0.074569;logMAE: train=-3.241278, val=-2.596027\n",
      "1400/10000 (epoch 701):Loss: train=0.038212, val=0.068454;logMAE: train=-3.264598, val=-2.681588\n",
      "1500/10000 (epoch 751):Loss: train=0.037137, val=0.062917;logMAE: train=-3.293150, val=-2.765937\n",
      "1600/10000 (epoch 801):Loss: train=0.035941, val=0.057834;logMAE: train=-3.325868, val=-2.850187\n",
      "1700/10000 (epoch 851):Loss: train=0.034540, val=0.053035;logMAE: train=-3.365623, val=-2.936800\n",
      "1800/10000 (epoch 901):Loss: train=0.032772, val=0.048793;logMAE: train=-3.418179, val=-3.020177\n",
      "1900/10000 (epoch 951):Loss: train=0.030646, val=0.044788;logMAE: train=-3.485269, val=-3.105805\n",
      "2000/10000 (epoch 1001):Loss: train=0.028000, val=0.040981;logMAE: train=-3.575551, val=-3.194643\n",
      "2100/10000 (epoch 1051):Loss: train=0.024548, val=0.037640;logMAE: train=-3.707123, val=-3.279682\n",
      "2200/10000 (epoch 1101):Loss: train=0.020219, val=0.034775;logMAE: train=-3.901147, val=-3.358867\n",
      "2300/10000 (epoch 1151):Loss: train=0.015031, val=0.032134;logMAE: train=-4.197658, val=-3.437838\n",
      "2400/10000 (epoch 1201):Loss: train=0.010410, val=0.029708;logMAE: train=-4.565005, val=-3.516343\n",
      "2500/10000 (epoch 1251):Loss: train=0.007508, val=0.027504;logMAE: train=-4.891792, val=-3.593406\n",
      "2600/10000 (epoch 1301):Loss: train=0.006497, val=0.025434;logMAE: train=-5.036475, val=-3.671654\n",
      "2700/10000 (epoch 1351):Loss: train=0.006385, val=0.023476;logMAE: train=-5.053795, val=-3.751769\n",
      "2800/10000 (epoch 1401):Loss: train=0.006379, val=0.021636;logMAE: train=-5.054798, val=-3.833402\n",
      "2900/10000 (epoch 1451):Loss: train=0.006378, val=0.019911;logMAE: train=-5.054932, val=-3.916466\n",
      "3000/10000 (epoch 1501):Loss: train=0.006378, val=0.018310;logMAE: train=-5.054888, val=-4.000296\n",
      "3100/10000 (epoch 1551):Loss: train=0.006387, val=0.016817;logMAE: train=-5.053491, val=-4.085342\n",
      "3200/10000 (epoch 1601):Loss: train=0.006381, val=0.015430;logMAE: train=-5.054426, val=-4.171421\n",
      "3300/10000 (epoch 1651):Loss: train=0.006379, val=0.014162;logMAE: train=-5.054731, val=-4.257197\n",
      "3400/10000 (epoch 1701):Loss: train=0.006377, val=0.013010;logMAE: train=-5.055068, val=-4.342045\n",
      "3500/10000 (epoch 1751):Loss: train=0.006387, val=0.011961;logMAE: train=-5.053445, val=-4.426113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Save progress\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m%\u001b[39m save_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     manager\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m     20\u001b[0m \u001b[39m# Evaluate model and log results\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m%\u001b[39m evaluation_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m     \u001b[39m# Save backup variables and load averaged variables\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint_management.py:817\u001b[0m, in \u001b[0;36mCheckpointManager.save\u001b[0;34m(self, checkpoint_number, check_interval, options)\u001b[0m\n\u001b[1;32m    815\u001b[0m prefix \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prefix, checkpoint_number)\n\u001b[1;32m    816\u001b[0m \u001b[39mif\u001b[39;00m options \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 817\u001b[0m   save_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_checkpoint\u001b[39m.\u001b[39;49mwrite(prefix)\n\u001b[1;32m    818\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m   save_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint\u001b[39m.\u001b[39mwrite(prefix, options\u001b[39m=\u001b[39moptions)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:2237\u001b[0m, in \u001b[0;36mCheckpoint.write\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(file_prefix, os\u001b[39m.\u001b[39mPathLike):\n\u001b[1;32m   2236\u001b[0m   file_prefix \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(file_prefix)\n\u001b[0;32m-> 2237\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_write(file_prefix, options)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:2256\u001b[0m, in \u001b[0;36mCheckpoint._write\u001b[0;34m(self, file_prefix, options, update_ckpt_state)\u001b[0m\n\u001b[1;32m   2254\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2255\u001b[0m options \u001b[39m=\u001b[39m options \u001b[39mor\u001b[39;00m checkpoint_options\u001b[39m.\u001b[39mCheckpointOptions()\n\u001b[0;32m-> 2256\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_saver\u001b[39m.\u001b[39;49msave(\n\u001b[1;32m   2257\u001b[0m     file_prefix\u001b[39m=\u001b[39;49mfile_prefix,\n\u001b[1;32m   2258\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   2259\u001b[0m     update_ckpt_state\u001b[39m=\u001b[39;49mupdate_ckpt_state)\n\u001b[1;32m   2260\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2262\u001b[0m metrics\u001b[39m.\u001b[39mAddCheckpointWriteDuration(\n\u001b[1;32m   2263\u001b[0m     api_label\u001b[39m=\u001b[39m_CHECKPOINT_V2,\n\u001b[1;32m   2264\u001b[0m     microseconds\u001b[39m=\u001b[39m_get_duration_microseconds(start_time, end_time))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:1351\u001b[0m, in \u001b[0;36mTrackableSaver.save\u001b[0;34m(self, file_prefix, checkpoint_number, session, options, update_ckpt_state)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tensor_util\u001b[39m.\u001b[39mis_tensor(file_prefix):\n\u001b[1;32m   1349\u001b[0m   file_io\u001b[39m.\u001b[39mrecursive_create_dir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(file_prefix))\n\u001b[0;32m-> 1351\u001b[0m save_path, new_feed_additions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_cached_when_graph_building(\n\u001b[1;32m   1352\u001b[0m     file_prefix_tensor, object_graph_tensor, options, update_ckpt_state)\n\u001b[1;32m   1354\u001b[0m \u001b[39mif\u001b[39;00m new_feed_additions:\n\u001b[1;32m   1355\u001b[0m   feed_dict\u001b[39m.\u001b[39mupdate(new_feed_additions)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:1296\u001b[0m, in \u001b[0;36mTrackableSaver._save_cached_when_graph_building\u001b[0;34m(self, file_prefix, object_graph_tensor, options, update_ckpt_state)\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_save_operation, feed_additions\n\u001b[1;32m   1295\u001b[0m \u001b[39m# Execute the normal checkpoint, i.e., synchronous.\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m \u001b[39mreturn\u001b[39;00m _run_save()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:1239\u001b[0m, in \u001b[0;36mTrackableSaver._save_cached_when_graph_building.<locals>._run_save\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_save_object_graph \u001b[39m!=\u001b[39m graph_proto\n\u001b[1;32m   1232\u001b[0m     \u001b[39m# When executing eagerly, we need to re-create SaveableObjects each\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     \u001b[39m# time save() is called so they pick up new Tensors passed to their\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m     \u001b[39m# constructors. That means the Saver needs to be copied with a new\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m     \u001b[39m# var_list.\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m     \u001b[39mor\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function()):\n\u001b[1;32m   1237\u001b[0m   saver \u001b[39m=\u001b[39m functional_saver\u001b[39m.\u001b[39mMultiDeviceSaver(named_saveable_objects,\n\u001b[1;32m   1238\u001b[0m                                             registered_savers)\n\u001b[0;32m-> 1239\u001b[0m   save_op \u001b[39m=\u001b[39m saver\u001b[39m.\u001b[39;49msave(file_prefix, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m   1240\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39m/cpu:0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1241\u001b[0m     \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcontrol_dependencies([save_op]):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/checkpoint/functional_saver.py:362\u001b[0m, in \u001b[0;36mMultiDeviceSaver.save\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    360\u001b[0m   tf_function_save()\n\u001b[1;32m    361\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 362\u001b[0m   \u001b[39mreturn\u001b[39;00m save_fn()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/checkpoint/functional_saver.py:348\u001b[0m, in \u001b[0;36mMultiDeviceSaver.save.<locals>.save_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    341\u001b[0m merge_device \u001b[39m=\u001b[39m (\n\u001b[1;32m    342\u001b[0m     options\u001b[39m.\u001b[39mexperimental_io_device \u001b[39mor\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     saveable_object_util\u001b[39m.\u001b[39mset_cpu0(last_device))\n\u001b[1;32m    344\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mdevice(merge_device):\n\u001b[1;32m    345\u001b[0m   \u001b[39m# V2 format write path consists of a metadata merge step.  Once\u001b[39;00m\n\u001b[1;32m    346\u001b[0m   \u001b[39m# merged, attempts to delete the temporary directory,\u001b[39;00m\n\u001b[1;32m    347\u001b[0m   \u001b[39m# \"<user-fed prefix>_temp\".\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_io_ops\u001b[39m.\u001b[39;49mmerge_v2_checkpoints(\n\u001b[1;32m    349\u001b[0m       saved_prefixes, file_prefix, delete_old_dirs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/ops/gen_io_ops.py:519\u001b[0m, in \u001b[0;36mmerge_v2_checkpoints\u001b[0;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, allow_missing_files, name)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 519\u001b[0m   \u001b[39mreturn\u001b[39;00m merge_v2_checkpoints_eager_fallback(\n\u001b[1;32m    520\u001b[0m       checkpoint_prefixes, destination_prefix,\n\u001b[1;32m    521\u001b[0m       delete_old_dirs\u001b[39m=\u001b[39;49mdelete_old_dirs,\n\u001b[1;32m    522\u001b[0m       allow_missing_files\u001b[39m=\u001b[39;49mallow_missing_files, name\u001b[39m=\u001b[39;49mname, ctx\u001b[39m=\u001b[39;49m_ctx)\n\u001b[1;32m    523\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_SymbolicException:\n\u001b[1;32m    524\u001b[0m   \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/ops/gen_io_ops.py:550\u001b[0m, in \u001b[0;36mmerge_v2_checkpoints_eager_fallback\u001b[0;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, allow_missing_files, name, ctx)\u001b[0m\n\u001b[1;32m    548\u001b[0m allow_missing_files \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39mmake_bool(allow_missing_files, \u001b[39m\"\u001b[39m\u001b[39mallow_missing_files\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    549\u001b[0m checkpoint_prefixes \u001b[39m=\u001b[39m _ops\u001b[39m.\u001b[39mconvert_to_tensor(checkpoint_prefixes, _dtypes\u001b[39m.\u001b[39mstring)\n\u001b[0;32m--> 550\u001b[0m destination_prefix \u001b[39m=\u001b[39m _ops\u001b[39m.\u001b[39;49mconvert_to_tensor(destination_prefix, _dtypes\u001b[39m.\u001b[39;49mstring)\n\u001b[1;32m    551\u001b[0m _inputs_flat \u001b[39m=\u001b[39m [checkpoint_prefixes, destination_prefix]\n\u001b[1;32m    552\u001b[0m _attrs \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mdelete_old_dirs\u001b[39m\u001b[39m\"\u001b[39m, delete_old_dirs, \u001b[39m\"\u001b[39m\u001b[39mallow_missing_files\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    553\u001b[0m allow_missing_files)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1598\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1596\u001b[0m   dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\n\u001b[1;32m   1597\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor):\n\u001b[0;32m-> 1598\u001b[0m   \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m dtype\u001b[39m.\u001b[39;49mis_compatible_with(value\u001b[39m.\u001b[39;49mdtype):\n\u001b[1;32m   1599\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1600\u001b[0m         _add_error_prefix(\n\u001b[1;32m   1601\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensor conversion requested dtype \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1602\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfor Tensor with dtype \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1603\u001b[0m             name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1604\u001b[0m   \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:188\u001b[0m, in \u001b[0;36mDType.is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_compatible_with\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m    173\u001b[0m   \u001b[39m\"\"\"Returns True if the `other` DType will be converted to this DType.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[39m  The conversion rules are as follows:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m    this `DType`.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m   other \u001b[39m=\u001b[39m as_dtype(other)\n\u001b[1;32m    189\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_type_enum \u001b[39min\u001b[39;00m (other\u001b[39m.\u001b[39mas_datatype_enum,\n\u001b[1;32m    190\u001b[0m                              other\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mas_datatype_enum)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:738\u001b[0m, in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdtypes.as_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mas_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    720\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mas_dtype\u001b[39m(type_value):\n\u001b[1;32m    721\u001b[0m   \u001b[39m\"\"\"Converts the given `type_value` to a `DType`.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \n\u001b[1;32m    723\u001b[0m \u001b[39m  Note: `DType` values are interned. When passed a new `DType` object,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m    TypeError: If `type_value` cannot be converted to a `DType`.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(type_value, DType):\n\u001b[1;32m    739\u001b[0m     \u001b[39mreturn\u001b[39;00m _INTERN_TABLE[type_value\u001b[39m.\u001b[39mas_datatype_enum]\n\u001b[1;32m    741\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(type_value, np\u001b[39m.\u001b[39mdtype):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "    else:\n",
    "        step_init = 1\n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # Perform training step\n",
    "        trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "                \n",
    "            epoch = step // steps_per_epoch\n",
    "            logging.info(\n",
    "                f\"{step}/{num_steps} (epoch {epoch + 1}):\"\n",
    "                f\"Loss: train={train['metrics'].loss:.6f}, val={validation['metrics'].loss:.6f};\"\n",
    "                f\"logMAE: train={train['metrics'].mean_log_mae:.6f}, \"\n",
    "                f\"val={validation['metrics'].mean_log_mae:.6f}\"\n",
    "            )\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
