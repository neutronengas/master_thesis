{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from model.dmnet import DMNet\n",
    "from model.activations import swish\n",
    "from training.metrics import Metrics\n",
    "from training.trainer import Trainer\n",
    "from training.data_container import DataContainer\n",
    "from training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../configs/config_dmnet.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "emb_size = config['emb_size']\n",
    "num_interaction_blocks = config['num_interaction_blocks']\n",
    "width_ticks = config['width_ticks']\n",
    "length_ticks = config['length_ticks']\n",
    "cutoff = config['cutoff']\n",
    "m_max = config[\"m_max\"]\n",
    "max_no_orbitals_per_m = config[\"max_no_orbitals_per_m\"]\n",
    "max_split_per_m = config[\"max_split_per_m\"]\n",
    "max_number_coeffs_per_ao = config[\"max_number_coeffs_per_ao\"]\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "target = config['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create directories***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 17:03:13 (INFO): Directory: ../logging/20240102_170313_dmnet_EIVI1RVH_formamide.npz_mp_1rdms_final\n"
     ]
    }
   ],
   "source": [
    "# Used for creating a random \"unique\" id for this run\n",
    "def id_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# Create directories\n",
    "# A unique directory name is created for this run based on the input\n",
    "if restart is None:\n",
    "    directory = (logdir + \"/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + model_name\n",
    "                 + \"_\" + id_generator()\n",
    "                 + \"_\" + os.path.basename(dataset)\n",
    "                 + \"_\" + '-'.join(target)\n",
    "                 + \"_\" + comment)\n",
    "else:\n",
    "    directory = restart\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "best_dir = os.path.join(directory, 'best')\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, 'logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "best_loss_file = os.path.join(best_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(best_dir, 'ckpt')\n",
    "step_ckpt_folder = log_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create summary writer and metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "train = {}\n",
    "validation = {}\n",
    "train['metrics'] = Metrics('train', target)\n",
    "validation['metrics'] = Metrics('val', target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 17:03:13.250673: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "data_container = DataContainer(dataset, target, cutoff, length_ticks * width_ticks)\n",
    "\n",
    "orbital_parameters = (m_max, max_no_orbitals_per_m, max_split_per_m, max_number_coeffs_per_ao)\n",
    "data_provider = DataProvider(data_container, width_ticks, length_ticks, num_train, num_valid, batch_size, seed=data_seed, randomized=True)\n",
    "\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"dmnet\":\n",
    "    model = DMNet(emb_size, num_interaction_blocks=num_interaction_blocks)\n",
    "else:\n",
    "    model = DMNet(emb_size, num_interaction_blocks=num_interaction_blocks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save/load best recorded loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/Documents/MA/models/dmnet/training/metrics.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.mean(np.log(self.maes)).item()\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(best_loss_file):\n",
    "    loss_file = np.load(best_loss_file)\n",
    "    metrics_best = {k: v.item() for k, v in loss_file.items()}\n",
    "else:\n",
    "    metrics_best = validation['metrics'].result()\n",
    "    for key in metrics_best.keys():\n",
    "        metrics_best[key] = np.inf\n",
    "    metrics_best['step'] = 0\n",
    "    np.savez(best_loss_file, **metrics_best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize trainer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps, decay_steps, decay_rate, ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set up checkpointing and load latest checkpoint*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dmnet/output/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dmnet/output/Reshape:0\", shape=(None, 14, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/dmnet/output/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dmnet/interaction/Reshape_2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dmnet/interaction/Reshape_1:0\", shape=(None, 14, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/dmnet/interaction/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dmnet/interaction/Reshape_5:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dmnet/interaction/Reshape_4:0\", shape=(None, 14, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/dmnet/interaction/Cast_1:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dmnet/interaction/Reshape_8:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dmnet/interaction/Reshape_7:0\", shape=(None, 14, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/dmnet/interaction/Cast_2:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2024-01-02 17:03:17 (INFO): 100/10000 (epoch 11):Loss: train=0.007759, val=0.007997;logMAE: train=-4.858944, val=-4.828721\n",
      "2024-01-02 17:03:19 (INFO): 200/10000 (epoch 21):Loss: train=0.006258, val=0.007818;logMAE: train=-5.073864, val=-4.851337\n",
      "2024-01-02 17:03:22 (INFO): 300/10000 (epoch 31):Loss: train=0.004060, val=0.007408;logMAE: train=-5.506527, val=-4.905143\n",
      "2024-01-02 17:03:24 (INFO): 400/10000 (epoch 41):Loss: train=0.002169, val=0.006795;logMAE: train=-6.133633, val=-4.991560\n",
      "2024-01-02 17:03:27 (INFO): 500/10000 (epoch 51):Loss: train=0.000932, val=0.006071;logMAE: train=-6.977674, val=-5.104310\n",
      "2024-01-02 17:03:29 (INFO): 600/10000 (epoch 61):Loss: train=0.000518, val=0.005343;logMAE: train=-7.565140, val=-5.231999\n",
      "2024-01-02 17:03:32 (INFO): 700/10000 (epoch 71):Loss: train=0.000419, val=0.004664;logMAE: train=-7.778585, val=-5.367904\n",
      "2024-01-02 17:03:35 (INFO): 800/10000 (epoch 81):Loss: train=0.000384, val=0.004055;logMAE: train=-7.865437, val=-5.507821\n",
      "2024-01-02 17:03:37 (INFO): 900/10000 (epoch 91):Loss: train=0.000382, val=0.003518;logMAE: train=-7.869122, val=-5.649795\n",
      "2024-01-02 17:03:40 (INFO): 1000/10000 (epoch 101):Loss: train=0.000382, val=0.003054;logMAE: train=-7.869280, val=-5.791154\n",
      "2024-01-02 17:03:42 (INFO): 1100/10000 (epoch 111):Loss: train=0.000382, val=0.002674;logMAE: train=-7.869203, val=-5.924086\n",
      "2024-01-02 17:03:45 (INFO): 1200/10000 (epoch 121):Loss: train=0.000382, val=0.002352;logMAE: train=-7.869183, val=-6.052577\n",
      "2024-01-02 17:03:47 (INFO): 1300/10000 (epoch 131):Loss: train=0.000382, val=0.002068;logMAE: train=-7.869174, val=-6.180972\n",
      "2024-01-02 17:03:50 (INFO): 1400/10000 (epoch 141):Loss: train=0.000382, val=0.001818;logMAE: train=-7.869240, val=-6.310110\n",
      "2024-01-02 17:03:52 (INFO): 1500/10000 (epoch 151):Loss: train=0.000382, val=0.001600;logMAE: train=-7.869260, val=-6.437546\n",
      "2024-01-02 17:03:55 (INFO): 1600/10000 (epoch 161):Loss: train=0.000382, val=0.001411;logMAE: train=-7.869192, val=-6.563168\n",
      "2024-01-02 17:03:57 (INFO): 1700/10000 (epoch 171):Loss: train=0.000382, val=0.001250;logMAE: train=-7.869127, val=-6.684583\n",
      "2024-01-02 17:04:00 (INFO): 1800/10000 (epoch 181):Loss: train=0.000382, val=0.001120;logMAE: train=-7.869091, val=-6.794426\n",
      "2024-01-02 17:04:02 (INFO): 1900/10000 (epoch 191):Loss: train=0.000382, val=0.001011;logMAE: train=-7.869059, val=-6.896334\n",
      "2024-01-02 17:04:05 (INFO): 2000/10000 (epoch 201):Loss: train=0.000382, val=0.000922;logMAE: train=-7.869112, val=-6.989028\n",
      "2024-01-02 17:04:07 (INFO): 2100/10000 (epoch 211):Loss: train=0.000382, val=0.000846;logMAE: train=-7.869138, val=-7.075387\n",
      "2024-01-02 17:04:10 (INFO): 2200/10000 (epoch 221):Loss: train=0.000382, val=0.000782;logMAE: train=-7.869030, val=-7.153389\n",
      "2024-01-02 17:04:13 (INFO): 2300/10000 (epoch 231):Loss: train=0.000382, val=0.000728;logMAE: train=-7.868972, val=-7.225422\n",
      "2024-01-02 17:04:16 (INFO): 2400/10000 (epoch 241):Loss: train=0.000382, val=0.000679;logMAE: train=-7.869045, val=-7.294889\n",
      "2024-01-02 17:04:18 (INFO): 2500/10000 (epoch 251):Loss: train=0.000382, val=0.000636;logMAE: train=-7.869046, val=-7.360619\n",
      "2024-01-02 17:04:21 (INFO): 2600/10000 (epoch 261):Loss: train=0.000382, val=0.000598;logMAE: train=-7.869101, val=-7.422168\n",
      "2024-01-02 17:04:23 (INFO): 2700/10000 (epoch 271):Loss: train=0.000382, val=0.000565;logMAE: train=-7.869027, val=-7.478688\n",
      "2024-01-02 17:04:26 (INFO): 2800/10000 (epoch 281):Loss: train=0.000382, val=0.000537;logMAE: train=-7.868946, val=-7.529855\n",
      "2024-01-02 17:04:29 (INFO): 2900/10000 (epoch 291):Loss: train=0.000382, val=0.000512;logMAE: train=-7.868909, val=-7.577716\n",
      "2024-01-02 17:04:31 (INFO): 3000/10000 (epoch 301):Loss: train=0.000382, val=0.000490;logMAE: train=-7.868919, val=-7.621574\n",
      "2024-01-02 17:04:33 (INFO): 3100/10000 (epoch 311):Loss: train=0.000382, val=0.000472;logMAE: train=-7.868935, val=-7.657492\n",
      "2024-01-02 17:04:36 (INFO): 3200/10000 (epoch 321):Loss: train=0.000382, val=0.000459;logMAE: train=-7.868992, val=-7.687266\n",
      "2024-01-02 17:04:39 (INFO): 3300/10000 (epoch 331):Loss: train=0.000382, val=0.000447;logMAE: train=-7.868988, val=-7.712774\n",
      "2024-01-02 17:04:41 (INFO): 3400/10000 (epoch 341):Loss: train=0.000382, val=0.000437;logMAE: train=-7.868993, val=-7.734960\n",
      "2024-01-02 17:04:44 (INFO): 3500/10000 (epoch 351):Loss: train=0.000382, val=0.000429;logMAE: train=-7.869100, val=-7.753825\n",
      "2024-01-02 17:04:46 (INFO): 3600/10000 (epoch 361):Loss: train=0.000382, val=0.000422;logMAE: train=-7.869043, val=-7.770237\n",
      "2024-01-02 17:04:49 (INFO): 3700/10000 (epoch 371):Loss: train=0.000382, val=0.000416;logMAE: train=-7.869035, val=-7.784204\n",
      "2024-01-02 17:04:51 (INFO): 3800/10000 (epoch 381):Loss: train=0.000382, val=0.000411;logMAE: train=-7.869115, val=-7.796148\n",
      "2024-01-02 17:04:54 (INFO): 3900/10000 (epoch 391):Loss: train=0.000382, val=0.000407;logMAE: train=-7.869068, val=-7.806595\n",
      "2024-01-02 17:04:56 (INFO): 4000/10000 (epoch 401):Loss: train=0.000382, val=0.000403;logMAE: train=-7.869032, val=-7.815845\n",
      "2024-01-02 17:04:59 (INFO): 4100/10000 (epoch 411):Loss: train=0.000382, val=0.000400;logMAE: train=-7.869052, val=-7.823827\n",
      "2024-01-02 17:05:02 (INFO): 4200/10000 (epoch 421):Loss: train=0.000382, val=0.000397;logMAE: train=-7.868946, val=-7.830361\n",
      "2024-01-02 17:05:04 (INFO): 4300/10000 (epoch 431):Loss: train=0.000382, val=0.000395;logMAE: train=-7.869090, val=-7.835747\n",
      "2024-01-02 17:05:07 (INFO): 4400/10000 (epoch 441):Loss: train=0.000382, val=0.000393;logMAE: train=-7.868954, val=-7.840434\n",
      "2024-01-02 17:05:09 (INFO): 4500/10000 (epoch 451):Loss: train=0.000382, val=0.000392;logMAE: train=-7.869094, val=-7.844519\n",
      "2024-01-02 17:05:12 (INFO): 4600/10000 (epoch 461):Loss: train=0.000382, val=0.000391;logMAE: train=-7.868976, val=-7.848081\n",
      "2024-01-02 17:05:14 (INFO): 4700/10000 (epoch 471):Loss: train=0.000382, val=0.000389;logMAE: train=-7.869106, val=-7.851218\n",
      "2024-01-02 17:05:17 (INFO): 4800/10000 (epoch 481):Loss: train=0.000382, val=0.000388;logMAE: train=-7.869071, val=-7.853858\n",
      "2024-01-02 17:05:19 (INFO): 4900/10000 (epoch 491):Loss: train=0.000382, val=0.000387;logMAE: train=-7.869010, val=-7.856076\n",
      "2024-01-02 17:05:22 (INFO): 5000/10000 (epoch 501):Loss: train=0.000382, val=0.000387;logMAE: train=-7.869121, val=-7.858009\n",
      "2024-01-02 17:05:24 (INFO): 5100/10000 (epoch 511):Loss: train=0.000382, val=0.000386;logMAE: train=-7.869132, val=-7.859746\n",
      "2024-01-02 17:05:27 (INFO): 5200/10000 (epoch 521):Loss: train=0.000382, val=0.000385;logMAE: train=-7.869120, val=-7.861305\n",
      "2024-01-02 17:05:29 (INFO): 5300/10000 (epoch 531):Loss: train=0.000382, val=0.000385;logMAE: train=-7.869083, val=-7.862760\n",
      "2024-01-02 17:05:32 (INFO): 5400/10000 (epoch 541):Loss: train=0.000382, val=0.000384;logMAE: train=-7.869079, val=-7.864079\n",
      "2024-01-02 17:05:35 (INFO): 5500/10000 (epoch 551):Loss: train=0.000382, val=0.000384;logMAE: train=-7.869080, val=-7.865241\n",
      "2024-01-02 17:05:37 (INFO): 5600/10000 (epoch 561):Loss: train=0.000382, val=0.000383;logMAE: train=-7.869016, val=-7.866243\n",
      "2024-01-02 17:05:40 (INFO): 5700/10000 (epoch 571):Loss: train=0.000382, val=0.000383;logMAE: train=-7.869155, val=-7.867076\n",
      "2024-01-02 17:05:42 (INFO): 5800/10000 (epoch 581):Loss: train=0.000382, val=0.000383;logMAE: train=-7.869121, val=-7.867784\n",
      "2024-01-02 17:05:45 (INFO): 5900/10000 (epoch 591):Loss: train=0.000382, val=0.000383;logMAE: train=-7.869136, val=-7.868351\n",
      "2024-01-02 17:05:47 (INFO): 6000/10000 (epoch 601):Loss: train=0.000382, val=0.000383;logMAE: train=-7.869215, val=-7.868728\n",
      "2024-01-02 17:05:50 (INFO): 6100/10000 (epoch 611):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869134, val=-7.869032\n",
      "2024-01-02 17:05:52 (INFO): 6200/10000 (epoch 621):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869142, val=-7.869292\n",
      "2024-01-02 17:05:55 (INFO): 6300/10000 (epoch 631):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869087, val=-7.869485\n",
      "2024-01-02 17:05:58 (INFO): 6400/10000 (epoch 641):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869187, val=-7.869637\n",
      "2024-01-02 17:06:00 (INFO): 6500/10000 (epoch 651):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869197, val=-7.869745\n",
      "2024-01-02 17:06:03 (INFO): 6600/10000 (epoch 661):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869179, val=-7.869823\n",
      "2024-01-02 17:06:05 (INFO): 6700/10000 (epoch 671):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869219, val=-7.869870\n",
      "2024-01-02 17:06:08 (INFO): 6800/10000 (epoch 681):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869164, val=-7.869895\n",
      "2024-01-02 17:06:10 (INFO): 6900/10000 (epoch 691):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869167, val=-7.869917\n",
      "2024-01-02 17:06:13 (INFO): 7000/10000 (epoch 701):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869216, val=-7.869934\n",
      "2024-01-02 17:06:16 (INFO): 7100/10000 (epoch 711):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869084, val=-7.869942\n",
      "2024-01-02 17:06:18 (INFO): 7200/10000 (epoch 721):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869136, val=-7.869952\n",
      "2024-01-02 17:06:21 (INFO): 7300/10000 (epoch 731):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869238, val=-7.869960\n",
      "2024-01-02 17:06:23 (INFO): 7400/10000 (epoch 741):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869176, val=-7.869961\n",
      "2024-01-02 17:06:26 (INFO): 7500/10000 (epoch 751):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869162, val=-7.869965\n",
      "2024-01-02 17:06:29 (INFO): 7600/10000 (epoch 761):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869208, val=-7.869969\n",
      "2024-01-02 17:06:31 (INFO): 7700/10000 (epoch 771):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869228, val=-7.869973\n",
      "2024-01-02 17:06:34 (INFO): 7800/10000 (epoch 781):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869200, val=-7.869975\n",
      "2024-01-02 17:06:36 (INFO): 7900/10000 (epoch 791):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869179, val=-7.869979\n",
      "2024-01-02 17:06:39 (INFO): 8000/10000 (epoch 801):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869218, val=-7.869985\n",
      "2024-01-02 17:06:41 (INFO): 8100/10000 (epoch 811):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869220, val=-7.869987\n",
      "2024-01-02 17:06:44 (INFO): 8200/10000 (epoch 821):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869196, val=-7.869988\n",
      "2024-01-02 17:06:47 (INFO): 8300/10000 (epoch 831):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869206, val=-7.869988\n",
      "2024-01-02 17:06:49 (INFO): 8400/10000 (epoch 841):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869211, val=-7.869987\n",
      "2024-01-02 17:06:52 (INFO): 8500/10000 (epoch 851):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869258, val=-7.869987\n",
      "2024-01-02 17:06:54 (INFO): 8600/10000 (epoch 861):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869205, val=-7.869985\n",
      "2024-01-02 17:06:57 (INFO): 8700/10000 (epoch 871):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869165, val=-7.869985\n",
      "2024-01-02 17:07:00 (INFO): 8800/10000 (epoch 881):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869279, val=-7.869987\n",
      "2024-01-02 17:07:02 (INFO): 8900/10000 (epoch 891):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869263, val=-7.869987\n",
      "2024-01-02 17:07:05 (INFO): 9000/10000 (epoch 901):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869290, val=-7.869987\n",
      "2024-01-02 17:07:07 (INFO): 9100/10000 (epoch 911):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869220, val=-7.869988\n",
      "2024-01-02 17:07:10 (INFO): 9200/10000 (epoch 921):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869216, val=-7.869991\n",
      "2024-01-02 17:07:12 (INFO): 9300/10000 (epoch 931):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869288, val=-7.869991\n",
      "2024-01-02 17:07:15 (INFO): 9400/10000 (epoch 941):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869255, val=-7.869990\n",
      "2024-01-02 17:07:18 (INFO): 9500/10000 (epoch 951):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869258, val=-7.869991\n",
      "2024-01-02 17:07:20 (INFO): 9600/10000 (epoch 961):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869291, val=-7.869989\n",
      "2024-01-02 17:07:23 (INFO): 9700/10000 (epoch 971):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869308, val=-7.869987\n",
      "2024-01-02 17:07:25 (INFO): 9800/10000 (epoch 981):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869240, val=-7.869988\n",
      "2024-01-02 17:07:28 (INFO): 9900/10000 (epoch 991):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869239, val=-7.869989\n",
      "2024-01-02 17:07:30 (INFO): 10000/10000 (epoch 1001):Loss: train=0.000382, val=0.000382;logMAE: train=-7.869254, val=-7.869987\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "    else:\n",
    "        step_init = 1\n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # Perform training step\n",
    "        trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "                \n",
    "            epoch = step // steps_per_epoch\n",
    "            logging.info(\n",
    "                f\"{step}/{num_steps} (epoch {epoch + 1}):\"\n",
    "                f\"Loss: train={train['metrics'].loss:.6f}, val={validation['metrics'].loss:.6f};\"\n",
    "                f\"logMAE: train={train['metrics'].mean_log_mae:.6f}, \"\n",
    "                f\"val={validation['metrics'].mean_log_mae:.6f}\"\n",
    "            )\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
