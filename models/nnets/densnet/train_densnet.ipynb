{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from model.densnet import DensNet\n",
    "from model.dumbnet import DumbNet\n",
    "from model.activations import swish\n",
    "from training.metrics import Metrics\n",
    "from training.trainer import Trainer\n",
    "from training.data_container import DataContainer\n",
    "from training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../configs/config_densnet.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "s_type_per_atom = config['s_type_per_atom']\n",
    "p_type_per_atom = config['p_type_per_atom']\n",
    "emb_size = config['emb_size']\n",
    "num_interaction_blocks = config['num_interaction_blocks']\n",
    "width_ticks = config['width_ticks']\n",
    "length_ticks = config['length_ticks']\n",
    "cutoff = config['cutoff']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "target = config['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create directories***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 14:12:32 (INFO): Directory: ../../logging/20231009_141232_densnet_XtBpMVxL_md_h2.npz_densities_final\n"
     ]
    }
   ],
   "source": [
    "# Used for creating a random \"unique\" id for this run\n",
    "def id_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# Create directories\n",
    "# A unique directory name is created for this run based on the input\n",
    "if restart is None:\n",
    "    directory = (\"../\" + logdir + \"/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + model_name\n",
    "                 + \"_\" + id_generator()\n",
    "                 + \"_\" + os.path.basename(dataset)\n",
    "                 + \"_\" + '-'.join(target)\n",
    "                 + \"_\" + comment)\n",
    "else:\n",
    "    directory = restart\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "best_dir = os.path.join(directory, 'best')\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, 'logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "best_loss_file = os.path.join(best_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(best_dir, 'ckpt')\n",
    "step_ckpt_folder = log_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create summary writer and metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "train = {}\n",
    "validation = {}\n",
    "train['metrics'] = Metrics('train', target)\n",
    "validation['metrics'] = Metrics('val', target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_container = DataContainer(dataset, target, cutoff)\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size, seed=data_seed, randomized=True)\n",
    "\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"dumbnet\":\n",
    "    model = None\n",
    "else:\n",
    "    model = DensNet(num_interaction_blocks=num_interaction_blocks, num_grid_points=width_ticks*length_ticks, emb_size=emb_size, s_type_per_atom=s_type_per_atom, p_type_per_atom=p_type_per_atom, activation=swish)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save/load best recorded loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(best_loss_file):\n",
    "    loss_file = np.load(best_loss_file)\n",
    "    metrics_best = {k: v.item() for k, v in loss_file.items()}\n",
    "else:\n",
    "    metrics_best = validation['metrics'].result()\n",
    "    for key in metrics_best.keys():\n",
    "        metrics_best[key] = np.inf\n",
    "    metrics_best['step'] = 0\n",
    "    np.savez(best_loss_file, **metrics_best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize trainer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps, decay_steps, decay_rate, ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set up checkpointing and load latest checkpoint*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 14:12:36 (INFO): 100/10000 (epoch 4):Loss: train=0.499737, val=1.104903;logMAE: train=-0.693673, val=0.099757\n",
      "2023-10-09 14:12:38 (INFO): 200/10000 (epoch 8):Loss: train=0.058403, val=0.966555;logMAE: train=-2.840384, val=-0.034017\n",
      "2023-10-09 14:12:41 (INFO): 300/10000 (epoch 12):Loss: train=0.032842, val=0.836789;logMAE: train=-3.416045, val=-0.178183\n",
      "2023-10-09 14:12:44 (INFO): 400/10000 (epoch 15):Loss: train=0.016356, val=0.734644;logMAE: train=-4.113172, val=-0.308369\n",
      "2023-10-09 14:12:46 (INFO): 500/10000 (epoch 19):Loss: train=0.015583, val=0.643530;logMAE: train=-4.161548, val=-0.440787\n",
      "2023-10-09 14:12:49 (INFO): 600/10000 (epoch 23):Loss: train=0.018584, val=0.557410;logMAE: train=-3.985455, val=-0.584454\n",
      "2023-10-09 14:12:53 (INFO): 700/10000 (epoch 26):Loss: train=0.018285, val=0.484553;logMAE: train=-4.001658, val=-0.724529\n",
      "2023-10-09 14:12:56 (INFO): 800/10000 (epoch 30):Loss: train=0.012898, val=0.426132;logMAE: train=-4.350688, val=-0.853005\n",
      "2023-10-09 14:12:58 (INFO): 900/10000 (epoch 34):Loss: train=0.015576, val=0.386460;logMAE: train=-4.161995, val=-0.950727\n",
      "2023-10-09 14:13:01 (INFO): 1000/10000 (epoch 38):Loss: train=0.020158, val=0.355585;logMAE: train=-3.904145, val=-1.033990\n",
      "2023-10-09 14:13:03 (INFO): 1100/10000 (epoch 41):Loss: train=0.019579, val=0.329831;logMAE: train=-3.933292, val=-1.109174\n",
      "2023-10-09 14:13:06 (INFO): 1200/10000 (epoch 45):Loss: train=0.012127, val=0.302707;logMAE: train=-4.412350, val=-1.194990\n",
      "2023-10-09 14:13:08 (INFO): 1300/10000 (epoch 49):Loss: train=0.013660, val=0.276512;logMAE: train=-4.293300, val=-1.285502\n",
      "2023-10-09 14:13:11 (INFO): 1400/10000 (epoch 52):Loss: train=0.011220, val=0.246126;logMAE: train=-4.490021, val=-1.401912\n",
      "2023-10-09 14:13:14 (INFO): 1500/10000 (epoch 56):Loss: train=0.017474, val=0.210697;logMAE: train=-4.047026, val=-1.557334\n",
      "2023-10-09 14:13:16 (INFO): 1600/10000 (epoch 60):Loss: train=0.022228, val=0.180577;logMAE: train=-3.806399, val=-1.711600\n",
      "2023-10-09 14:13:19 (INFO): 1700/10000 (epoch 63):Loss: train=0.011330, val=0.165458;logMAE: train=-4.480330, val=-1.799039\n",
      "2023-10-09 14:13:21 (INFO): 1800/10000 (epoch 67):Loss: train=0.010393, val=0.151819;logMAE: train=-4.566668, val=-1.885067\n",
      "2023-10-09 14:13:24 (INFO): 1900/10000 (epoch 71):Loss: train=0.037207, val=0.141540;logMAE: train=-3.291251, val=-1.955171\n",
      "2023-10-09 14:13:26 (INFO): 2000/10000 (epoch 75):Loss: train=0.017743, val=0.135549;logMAE: train=-4.031764, val=-1.998423\n",
      "2023-10-09 14:13:29 (INFO): 2100/10000 (epoch 78):Loss: train=0.009219, val=0.128673;logMAE: train=-4.686537, val=-2.050483\n",
      "2023-10-09 14:13:31 (INFO): 2200/10000 (epoch 82):Loss: train=0.013170, val=0.117221;logMAE: train=-4.329843, val=-2.143691\n",
      "2023-10-09 14:13:34 (INFO): 2300/10000 (epoch 86):Loss: train=0.016112, val=0.107010;logMAE: train=-4.128193, val=-2.234833\n",
      "2023-10-09 14:13:36 (INFO): 2400/10000 (epoch 89):Loss: train=0.014055, val=0.096948;logMAE: train=-4.264804, val=-2.333581\n",
      "2023-10-09 14:13:39 (INFO): 2500/10000 (epoch 93):Loss: train=0.010219, val=0.088080;logMAE: train=-4.583543, val=-2.429505\n",
      "2023-10-09 14:13:41 (INFO): 2600/10000 (epoch 97):Loss: train=0.016450, val=0.078689;logMAE: train=-4.107439, val=-2.542252\n",
      "2023-10-09 14:13:44 (INFO): 2700/10000 (epoch 101):Loss: train=0.012142, val=0.078280;logMAE: train=-4.411116, val=-2.547459\n",
      "2023-10-09 14:13:46 (INFO): 2800/10000 (epoch 104):Loss: train=0.015350, val=0.072473;logMAE: train=-4.176613, val=-2.624536\n",
      "2023-10-09 14:13:49 (INFO): 2900/10000 (epoch 108):Loss: train=0.016483, val=0.068331;logMAE: train=-4.105418, val=-2.683389\n",
      "2023-10-09 14:13:51 (INFO): 3000/10000 (epoch 112):Loss: train=0.016726, val=0.055184;logMAE: train=-4.090809, val=-2.897082\n",
      "2023-10-09 14:13:54 (INFO): 3100/10000 (epoch 115):Loss: train=0.013996, val=0.041991;logMAE: train=-4.268981, val=-3.170309\n",
      "2023-10-09 14:13:56 (INFO): 3200/10000 (epoch 119):Loss: train=0.013874, val=0.030602;logMAE: train=-4.277767, val=-3.486694\n",
      "2023-10-09 14:13:59 (INFO): 3300/10000 (epoch 123):Loss: train=0.011960, val=0.025300;logMAE: train=-4.426188, val=-3.676953\n",
      "2023-10-09 14:14:01 (INFO): 3400/10000 (epoch 126):Loss: train=0.012914, val=0.022144;logMAE: train=-4.349464, val=-3.810205\n",
      "2023-10-09 14:14:04 (INFO): 3500/10000 (epoch 130):Loss: train=0.008396, val=0.018852;logMAE: train=-4.779981, val=-3.971119\n",
      "2023-10-09 14:14:06 (INFO): 3600/10000 (epoch 134):Loss: train=0.012131, val=0.016082;logMAE: train=-4.412018, val=-4.130037\n",
      "2023-10-09 14:14:09 (INFO): 3700/10000 (epoch 138):Loss: train=0.009208, val=0.013671;logMAE: train=-4.687634, val=-4.292491\n",
      "2023-10-09 14:14:11 (INFO): 3800/10000 (epoch 141):Loss: train=0.009215, val=0.011689;logMAE: train=-4.686962, val=-4.449122\n",
      "2023-10-09 14:14:14 (INFO): 3900/10000 (epoch 145):Loss: train=0.008536, val=0.010167;logMAE: train=-4.763411, val=-4.588603\n",
      "2023-10-09 14:14:16 (INFO): 4000/10000 (epoch 149):Loss: train=0.011639, val=0.009203;logMAE: train=-4.453419, val=-4.688234\n",
      "2023-10-09 14:14:19 (INFO): 4100/10000 (epoch 152):Loss: train=0.010976, val=0.008586;logMAE: train=-4.512084, val=-4.757666\n",
      "2023-10-09 14:14:22 (INFO): 4200/10000 (epoch 156):Loss: train=0.010419, val=0.008356;logMAE: train=-4.564125, val=-4.784736\n",
      "2023-10-09 14:14:24 (INFO): 4300/10000 (epoch 160):Loss: train=0.009841, val=0.008284;logMAE: train=-4.621191, val=-4.793466\n",
      "2023-10-09 14:14:27 (INFO): 4400/10000 (epoch 163):Loss: train=0.009836, val=0.008208;logMAE: train=-4.621667, val=-4.802603\n",
      "2023-10-09 14:14:29 (INFO): 4500/10000 (epoch 167):Loss: train=0.010476, val=0.008164;logMAE: train=-4.558662, val=-4.807977\n",
      "2023-10-09 14:14:32 (INFO): 4600/10000 (epoch 171):Loss: train=0.008009, val=0.008087;logMAE: train=-4.827170, val=-4.817509\n",
      "2023-10-09 14:14:34 (INFO): 4700/10000 (epoch 175):Loss: train=0.008097, val=0.007988;logMAE: train=-4.816281, val=-4.829757\n",
      "2023-10-09 14:14:37 (INFO): 4800/10000 (epoch 178):Loss: train=0.009754, val=0.007912;logMAE: train=-4.630081, val=-4.839372\n",
      "2023-10-09 14:14:39 (INFO): 4900/10000 (epoch 182):Loss: train=0.010966, val=0.007870;logMAE: train=-4.512963, val=-4.844651\n",
      "2023-10-09 14:14:42 (INFO): 5000/10000 (epoch 186):Loss: train=0.009330, val=0.007830;logMAE: train=-4.674535, val=-4.849841\n",
      "2023-10-09 14:14:45 (INFO): 5100/10000 (epoch 189):Loss: train=0.010403, val=0.007794;logMAE: train=-4.565662, val=-4.854405\n",
      "2023-10-09 14:14:47 (INFO): 5200/10000 (epoch 193):Loss: train=0.009507, val=0.007759;logMAE: train=-4.655715, val=-4.858944\n",
      "2023-10-09 14:14:50 (INFO): 5300/10000 (epoch 197):Loss: train=0.009232, val=0.007733;logMAE: train=-4.685096, val=-4.862247\n",
      "2023-10-09 14:14:52 (INFO): 5400/10000 (epoch 201):Loss: train=0.010012, val=0.007707;logMAE: train=-4.603967, val=-4.865602\n",
      "2023-10-09 14:14:55 (INFO): 5500/10000 (epoch 204):Loss: train=0.008956, val=0.007673;logMAE: train=-4.715409, val=-4.870086\n",
      "2023-10-09 14:14:57 (INFO): 5600/10000 (epoch 208):Loss: train=0.009006, val=0.007642;logMAE: train=-4.709862, val=-4.874042\n",
      "2023-10-09 14:15:00 (INFO): 5700/10000 (epoch 212):Loss: train=0.008151, val=0.007599;logMAE: train=-4.809560, val=-4.879674\n",
      "2023-10-09 14:15:02 (INFO): 5800/10000 (epoch 215):Loss: train=0.010224, val=0.007565;logMAE: train=-4.582982, val=-4.884203\n",
      "2023-10-09 14:15:05 (INFO): 5900/10000 (epoch 219):Loss: train=0.008374, val=0.007523;logMAE: train=-4.782668, val=-4.889818\n",
      "2023-10-09 14:15:07 (INFO): 6000/10000 (epoch 223):Loss: train=0.009525, val=0.007503;logMAE: train=-4.653792, val=-4.892515\n",
      "2023-10-09 14:15:10 (INFO): 6100/10000 (epoch 226):Loss: train=0.012044, val=0.007573;logMAE: train=-4.419186, val=-4.883220\n",
      "2023-10-09 14:15:12 (INFO): 6200/10000 (epoch 230):Loss: train=0.009405, val=0.007738;logMAE: train=-4.666501, val=-4.861587\n",
      "2023-10-09 14:15:15 (INFO): 6300/10000 (epoch 234):Loss: train=0.009138, val=0.007954;logMAE: train=-4.695318, val=-4.834130\n",
      "2023-10-09 14:15:17 (INFO): 6400/10000 (epoch 238):Loss: train=0.009117, val=0.008089;logMAE: train=-4.697635, val=-4.817224\n",
      "2023-10-09 14:15:20 (INFO): 6500/10000 (epoch 241):Loss: train=0.008864, val=0.008315;logMAE: train=-4.725745, val=-4.789641\n",
      "2023-10-09 14:15:22 (INFO): 6600/10000 (epoch 245):Loss: train=0.007921, val=0.008591;logMAE: train=-4.838182, val=-4.757016\n",
      "2023-10-09 14:15:25 (INFO): 6700/10000 (epoch 249):Loss: train=0.008353, val=0.008679;logMAE: train=-4.785085, val=-4.746877\n",
      "2023-10-09 14:15:27 (INFO): 6800/10000 (epoch 252):Loss: train=0.007752, val=0.008363;logMAE: train=-4.859869, val=-4.783983\n",
      "2023-10-09 14:15:29 (INFO): 6900/10000 (epoch 256):Loss: train=0.009126, val=0.008110;logMAE: train=-4.696682, val=-4.814716\n",
      "2023-10-09 14:15:32 (INFO): 7000/10000 (epoch 260):Loss: train=0.008513, val=0.007940;logMAE: train=-4.766108, val=-4.835860\n",
      "2023-10-09 14:15:34 (INFO): 7100/10000 (epoch 263):Loss: train=0.009111, val=0.007802;logMAE: train=-4.698263, val=-4.853371\n",
      "2023-10-09 14:15:37 (INFO): 7200/10000 (epoch 267):Loss: train=0.008417, val=0.007675;logMAE: train=-4.777481, val=-4.869737\n",
      "2023-10-09 14:15:39 (INFO): 7300/10000 (epoch 271):Loss: train=0.008152, val=0.007566;logMAE: train=-4.809512, val=-4.884126\n",
      "2023-10-09 14:15:42 (INFO): 7400/10000 (epoch 275):Loss: train=0.010056, val=0.007470;logMAE: train=-4.599616, val=-4.896839\n",
      "2023-10-09 14:15:44 (INFO): 7500/10000 (epoch 278):Loss: train=0.008180, val=0.007387;logMAE: train=-4.806088, val=-4.908057\n",
      "2023-10-09 14:15:47 (INFO): 7600/10000 (epoch 282):Loss: train=0.007706, val=0.007314;logMAE: train=-4.865725, val=-4.917954\n",
      "2023-10-09 14:15:49 (INFO): 7700/10000 (epoch 286):Loss: train=0.008064, val=0.007251;logMAE: train=-4.820348, val=-4.926678\n",
      "2023-10-09 14:15:52 (INFO): 7800/10000 (epoch 289):Loss: train=0.009135, val=0.007191;logMAE: train=-4.695631, val=-4.934876\n",
      "2023-10-09 14:15:54 (INFO): 7900/10000 (epoch 293):Loss: train=0.009239, val=0.007206;logMAE: train=-4.684300, val=-4.932897\n",
      "2023-10-09 14:15:57 (INFO): 8000/10000 (epoch 297):Loss: train=0.009869, val=0.007295;logMAE: train=-4.618406, val=-4.920504\n",
      "2023-10-09 14:15:59 (INFO): 8100/10000 (epoch 301):Loss: train=0.008292, val=0.007267;logMAE: train=-4.792468, val=-4.924463\n",
      "2023-10-09 14:16:02 (INFO): 8200/10000 (epoch 304):Loss: train=0.009368, val=0.007226;logMAE: train=-4.670471, val=-4.930120\n",
      "2023-10-09 14:16:04 (INFO): 8300/10000 (epoch 308):Loss: train=0.008597, val=0.007181;logMAE: train=-4.756345, val=-4.936281\n",
      "2023-10-09 14:16:07 (INFO): 8400/10000 (epoch 312):Loss: train=0.008970, val=0.007210;logMAE: train=-4.713896, val=-4.932329\n",
      "2023-10-09 14:16:09 (INFO): 8500/10000 (epoch 315):Loss: train=0.008300, val=0.007251;logMAE: train=-4.791496, val=-4.926602\n",
      "2023-10-09 14:16:12 (INFO): 8600/10000 (epoch 319):Loss: train=0.008978, val=0.007198;logMAE: train=-4.713022, val=-4.933925\n",
      "2023-10-09 14:16:14 (INFO): 8700/10000 (epoch 323):Loss: train=0.007628, val=0.007154;logMAE: train=-4.875905, val=-4.940084\n",
      "2023-10-09 14:16:17 (INFO): 8800/10000 (epoch 326):Loss: train=0.007841, val=0.007120;logMAE: train=-4.848437, val=-4.944852\n",
      "2023-10-09 14:16:19 (INFO): 8900/10000 (epoch 330):Loss: train=0.008488, val=0.007097;logMAE: train=-4.769048, val=-4.948145\n",
      "2023-10-09 14:16:22 (INFO): 9000/10000 (epoch 334):Loss: train=0.008132, val=0.007071;logMAE: train=-4.811923, val=-4.951800\n",
      "2023-10-09 14:16:24 (INFO): 9100/10000 (epoch 338):Loss: train=0.008313, val=0.007051;logMAE: train=-4.789924, val=-4.954571\n",
      "2023-10-09 14:16:27 (INFO): 9200/10000 (epoch 341):Loss: train=0.008428, val=0.007034;logMAE: train=-4.776199, val=-4.957052\n",
      "2023-10-09 14:16:29 (INFO): 9300/10000 (epoch 345):Loss: train=0.007645, val=0.007016;logMAE: train=-4.873676, val=-4.959584\n",
      "2023-10-09 14:16:32 (INFO): 9400/10000 (epoch 349):Loss: train=0.008252, val=0.007002;logMAE: train=-4.797350, val=-4.961551\n",
      "2023-10-09 14:16:34 (INFO): 9500/10000 (epoch 352):Loss: train=0.008021, val=0.006988;logMAE: train=-4.825675, val=-4.963510\n",
      "2023-10-09 14:16:37 (INFO): 9600/10000 (epoch 356):Loss: train=0.008815, val=0.006979;logMAE: train=-4.731262, val=-4.964804\n",
      "2023-10-09 14:16:39 (INFO): 9700/10000 (epoch 360):Loss: train=0.007710, val=0.006968;logMAE: train=-4.865202, val=-4.966428\n",
      "2023-10-09 14:16:42 (INFO): 9800/10000 (epoch 363):Loss: train=0.007769, val=0.006958;logMAE: train=-4.857665, val=-4.967916\n",
      "2023-10-09 14:16:44 (INFO): 9900/10000 (epoch 367):Loss: train=0.008033, val=0.006948;logMAE: train=-4.824244, val=-4.969311\n",
      "2023-10-09 14:16:47 (INFO): 10000/10000 (epoch 371):Loss: train=0.008353, val=0.006939;logMAE: train=-4.785093, val=-4.970622\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "    else:\n",
    "        step_init = 1\n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # Perform training step\n",
    "        trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "                \n",
    "            epoch = step // steps_per_epoch\n",
    "            logging.info(\n",
    "                f\"{step}/{num_steps} (epoch {epoch + 1}):\"\n",
    "                f\"Loss: train={train['metrics'].loss:.6f}, val={validation['metrics'].loss:.6f};\"\n",
    "                f\"logMAE: train={train['metrics'].mean_log_mae:.6f}, \"\n",
    "                f\"val={validation['metrics'].mean_log_mae:.6f}\"\n",
    "            )\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
