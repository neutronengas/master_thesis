{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benni/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from convnet.model.convnet import ConvNet\n",
    "from convnet.model.dumbnet import DumbNet\n",
    "from convnet.model.activations import swish\n",
    "from convnet.training.metrics import Metrics\n",
    "from convnet.training.trainer import Trainer\n",
    "from convnet.training.data_container import DataContainer\n",
    "from convnet.training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_convnet.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "target = config['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create directories***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 13:21:24 (INFO): Directory: ./logging/20230912_132124_convnet_JZrMddsl_md_h2.npz_corrs_final\n"
     ]
    }
   ],
   "source": [
    "# Used for creating a random \"unique\" id for this run\n",
    "def id_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# Create directories\n",
    "# A unique directory name is created for this run based on the input\n",
    "if restart is None:\n",
    "    directory = (logdir + \"/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + model_name\n",
    "                 + \"_\" + id_generator()\n",
    "                 + \"_\" + os.path.basename(dataset)\n",
    "                 + \"_\" + '-'.join(target)\n",
    "                 + \"_\" + comment)\n",
    "else:\n",
    "    directory = restart\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "best_dir = os.path.join(directory, 'best')\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, 'logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "best_loss_file = os.path.join(best_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(best_dir, 'ckpt')\n",
    "step_ckpt_folder = log_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create summary writer and metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "train = {}\n",
    "validation = {}\n",
    "train['metrics'] = Metrics('train', target)\n",
    "validation['metrics'] = Metrics('val', target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9600, 5, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "data_container = DataContainer(dataset, target)\n",
    "\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size, seed=data_seed, randomized=True)\n",
    "\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(activation=swish) if \"dumb\" not in model_name else DumbNet(activation=swish)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save/load best recorded loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benni/Dokumente/MA/master_thesis/models/nnets/convnet/training/metrics.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.mean(np.log(self.maes)).item()\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(best_loss_file):\n",
    "    loss_file = np.load(best_loss_file)\n",
    "    metrics_best = {k: v.item() for k, v in loss_file.items()}\n",
    "else:\n",
    "    metrics_best = validation['metrics'].result()\n",
    "    for key in metrics_best.keys():\n",
    "        metrics_best[key] = np.inf\n",
    "    metrics_best['step'] = 0\n",
    "    np.savez(best_loss_file, **metrics_best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize trainer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps, decay_steps, decay_rate, ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set up checkpointing and load latest checkpoint*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"convnet/conv2d_19/Relu:0\", shape=(None, 5, 5, 1), dtype=float32)\n",
      "Tensor(\"convnet/conv2d_19/Relu:0\", shape=(None, 5, 5, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 13:21:37 (INFO): 100/10000 (epoch 4):Loss: train=0.007672, val=0.008211;logMAE: train=-4.870242, val=-4.802312\n",
      "2023-09-12 13:21:44 (INFO): 200/10000 (epoch 7):Loss: train=0.004986, val=0.007871;logMAE: train=-5.301023, val=-4.844545\n",
      "2023-09-12 13:21:51 (INFO): 300/10000 (epoch 11):Loss: train=0.004212, val=0.007373;logMAE: train=-5.469820, val=-4.909982\n",
      "2023-09-12 13:21:58 (INFO): 400/10000 (epoch 14):Loss: train=0.004188, val=0.006886;logMAE: train=-5.475633, val=-4.978206\n",
      "2023-09-12 13:22:05 (INFO): 500/10000 (epoch 17):Loss: train=0.004213, val=0.006443;logMAE: train=-5.469691, val=-5.044838\n",
      "2023-09-12 13:22:12 (INFO): 600/10000 (epoch 21):Loss: train=0.004197, val=0.006047;logMAE: train=-5.473448, val=-5.108152\n",
      "2023-09-12 13:22:19 (INFO): 700/10000 (epoch 24):Loss: train=0.004192, val=0.005693;logMAE: train=-5.474553, val=-5.168601\n",
      "2023-09-12 13:22:26 (INFO): 800/10000 (epoch 27):Loss: train=0.004230, val=0.005393;logMAE: train=-5.465578, val=-5.222576\n",
      "2023-09-12 13:22:33 (INFO): 900/10000 (epoch 31):Loss: train=0.004177, val=0.005148;logMAE: train=-5.478181, val=-5.269072\n",
      "2023-09-12 13:22:40 (INFO): 1000/10000 (epoch 34):Loss: train=0.004222, val=0.004950;logMAE: train=-5.467547, val=-5.308404\n",
      "2023-09-12 13:22:47 (INFO): 1100/10000 (epoch 37):Loss: train=0.004166, val=0.004794;logMAE: train=-5.480725, val=-5.340461\n",
      "2023-09-12 13:22:54 (INFO): 1200/10000 (epoch 41):Loss: train=0.004221, val=0.004672;logMAE: train=-5.467608, val=-5.366240\n",
      "2023-09-12 13:23:01 (INFO): 1300/10000 (epoch 44):Loss: train=0.004217, val=0.004574;logMAE: train=-5.468708, val=-5.387451\n",
      "2023-09-12 13:23:08 (INFO): 1400/10000 (epoch 47):Loss: train=0.004207, val=0.004496;logMAE: train=-5.471070, val=-5.404560\n",
      "2023-09-12 13:23:15 (INFO): 1500/10000 (epoch 51):Loss: train=0.004192, val=0.004436;logMAE: train=-5.474538, val=-5.418089\n",
      "2023-09-12 13:23:23 (INFO): 1600/10000 (epoch 54):Loss: train=0.004233, val=0.004386;logMAE: train=-5.464932, val=-5.429360\n",
      "2023-09-12 13:23:30 (INFO): 1700/10000 (epoch 57):Loss: train=0.004177, val=0.004348;logMAE: train=-5.478277, val=-5.437948\n",
      "2023-09-12 13:23:37 (INFO): 1800/10000 (epoch 61):Loss: train=0.004215, val=0.004317;logMAE: train=-5.469212, val=-5.445137\n",
      "2023-09-12 13:23:44 (INFO): 1900/10000 (epoch 64):Loss: train=0.004181, val=0.004292;logMAE: train=-5.477181, val=-5.450918\n",
      "2023-09-12 13:23:51 (INFO): 2000/10000 (epoch 67):Loss: train=0.004240, val=0.004273;logMAE: train=-5.463197, val=-5.455429\n",
      "2023-09-12 13:23:58 (INFO): 2100/10000 (epoch 71):Loss: train=0.004208, val=0.004257;logMAE: train=-5.470816, val=-5.459188\n",
      "2023-09-12 13:24:05 (INFO): 2200/10000 (epoch 74):Loss: train=0.004247, val=0.004246;logMAE: train=-5.461528, val=-5.461865\n",
      "2023-09-12 13:24:12 (INFO): 2300/10000 (epoch 77):Loss: train=0.004197, val=0.004236;logMAE: train=-5.473388, val=-5.464163\n",
      "2023-09-12 13:24:19 (INFO): 2400/10000 (epoch 81):Loss: train=0.004187, val=0.004228;logMAE: train=-5.475743, val=-5.466042\n",
      "2023-09-12 13:24:27 (INFO): 2500/10000 (epoch 84):Loss: train=0.004196, val=0.004222;logMAE: train=-5.473618, val=-5.467428\n",
      "2023-09-12 13:24:34 (INFO): 2600/10000 (epoch 87):Loss: train=0.004224, val=0.004217;logMAE: train=-5.466935, val=-5.468514\n",
      "2023-09-12 13:24:41 (INFO): 2700/10000 (epoch 91):Loss: train=0.004211, val=0.004214;logMAE: train=-5.470022, val=-5.469327\n",
      "2023-09-12 13:24:48 (INFO): 2800/10000 (epoch 94):Loss: train=0.004205, val=0.004211;logMAE: train=-5.471386, val=-5.470061\n",
      "2023-09-12 13:24:55 (INFO): 2900/10000 (epoch 97):Loss: train=0.004208, val=0.004209;logMAE: train=-5.470860, val=-5.470574\n",
      "2023-09-12 13:25:02 (INFO): 3000/10000 (epoch 101):Loss: train=0.004232, val=0.004207;logMAE: train=-5.465069, val=-5.471007\n",
      "2023-09-12 13:25:09 (INFO): 3100/10000 (epoch 104):Loss: train=0.004215, val=0.004206;logMAE: train=-5.468987, val=-5.471348\n",
      "2023-09-12 13:25:17 (INFO): 3200/10000 (epoch 107):Loss: train=0.004186, val=0.004205;logMAE: train=-5.476118, val=-5.471589\n",
      "2023-09-12 13:25:24 (INFO): 3300/10000 (epoch 111):Loss: train=0.004228, val=0.004204;logMAE: train=-5.466014, val=-5.471779\n",
      "2023-09-12 13:25:31 (INFO): 3400/10000 (epoch 114):Loss: train=0.004179, val=0.004203;logMAE: train=-5.477780, val=-5.471916\n",
      "2023-09-12 13:25:38 (INFO): 3500/10000 (epoch 117):Loss: train=0.004279, val=0.004203;logMAE: train=-5.454024, val=-5.472055\n",
      "2023-09-12 13:25:45 (INFO): 3600/10000 (epoch 121):Loss: train=0.004211, val=0.004202;logMAE: train=-5.470098, val=-5.472105\n",
      "2023-09-12 13:25:53 (INFO): 3700/10000 (epoch 124):Loss: train=0.004230, val=0.004202;logMAE: train=-5.465576, val=-5.472135\n",
      "2023-09-12 13:26:00 (INFO): 3800/10000 (epoch 127):Loss: train=0.004192, val=0.004202;logMAE: train=-5.474664, val=-5.472140\n",
      "2023-09-12 13:26:07 (INFO): 3900/10000 (epoch 131):Loss: train=0.004232, val=0.004202;logMAE: train=-5.465050, val=-5.472126\n",
      "2023-09-12 13:26:14 (INFO): 4000/10000 (epoch 134):Loss: train=0.004218, val=0.004202;logMAE: train=-5.468332, val=-5.472103\n",
      "2023-09-12 13:26:20 (INFO): 4100/10000 (epoch 137):Loss: train=0.004207, val=0.004202;logMAE: train=-5.471069, val=-5.472088\n",
      "2023-09-12 13:26:27 (INFO): 4200/10000 (epoch 141):Loss: train=0.004214, val=0.004203;logMAE: train=-5.469416, val=-5.472049\n",
      "2023-09-12 13:26:34 (INFO): 4300/10000 (epoch 144):Loss: train=0.004235, val=0.004203;logMAE: train=-5.464488, val=-5.471995\n",
      "2023-09-12 13:26:41 (INFO): 4400/10000 (epoch 147):Loss: train=0.004197, val=0.004203;logMAE: train=-5.473334, val=-5.471997\n",
      "2023-09-12 13:26:48 (INFO): 4500/10000 (epoch 151):Loss: train=0.004224, val=0.004203;logMAE: train=-5.466873, val=-5.471949\n",
      "2023-09-12 13:26:55 (INFO): 4600/10000 (epoch 154):Loss: train=0.004210, val=0.004203;logMAE: train=-5.470360, val=-5.471899\n",
      "2023-09-12 13:27:03 (INFO): 4700/10000 (epoch 157):Loss: train=0.004206, val=0.004203;logMAE: train=-5.471248, val=-5.471881\n",
      "2023-09-12 13:27:13 (INFO): 4800/10000 (epoch 161):Loss: train=0.004208, val=0.004203;logMAE: train=-5.470665, val=-5.471871\n",
      "2023-09-12 13:27:20 (INFO): 4900/10000 (epoch 164):Loss: train=0.004236, val=0.004204;logMAE: train=-5.464215, val=-5.471837\n",
      "2023-09-12 13:27:27 (INFO): 5000/10000 (epoch 167):Loss: train=0.004180, val=0.004204;logMAE: train=-5.477342, val=-5.471808\n",
      "2023-09-12 13:27:35 (INFO): 5100/10000 (epoch 171):Loss: train=0.004229, val=0.004204;logMAE: train=-5.465709, val=-5.471813\n",
      "2023-09-12 13:27:42 (INFO): 5200/10000 (epoch 174):Loss: train=0.004250, val=0.004204;logMAE: train=-5.460901, val=-5.471797\n",
      "2023-09-12 13:27:50 (INFO): 5300/10000 (epoch 177):Loss: train=0.004199, val=0.004204;logMAE: train=-5.473010, val=-5.471766\n",
      "2023-09-12 13:27:57 (INFO): 5400/10000 (epoch 181):Loss: train=0.004213, val=0.004204;logMAE: train=-5.469695, val=-5.471766\n",
      "2023-09-12 13:28:05 (INFO): 5500/10000 (epoch 184):Loss: train=0.004191, val=0.004204;logMAE: train=-5.474704, val=-5.471746\n",
      "2023-09-12 13:28:12 (INFO): 5600/10000 (epoch 187):Loss: train=0.004246, val=0.004204;logMAE: train=-5.461800, val=-5.471736\n",
      "2023-09-12 13:28:19 (INFO): 5700/10000 (epoch 191):Loss: train=0.004200, val=0.004204;logMAE: train=-5.472654, val=-5.471744\n",
      "2023-09-12 13:28:27 (INFO): 5800/10000 (epoch 194):Loss: train=0.004233, val=0.004204;logMAE: train=-5.464870, val=-5.471731\n",
      "2023-09-12 13:28:34 (INFO): 5900/10000 (epoch 197):Loss: train=0.004191, val=0.004204;logMAE: train=-5.474743, val=-5.471727\n",
      "2023-09-12 13:28:41 (INFO): 6000/10000 (epoch 201):Loss: train=0.004256, val=0.004204;logMAE: train=-5.459526, val=-5.471705\n",
      "2023-09-12 13:28:48 (INFO): 6100/10000 (epoch 204):Loss: train=0.004197, val=0.004204;logMAE: train=-5.473399, val=-5.471706\n",
      "2023-09-12 13:28:55 (INFO): 6200/10000 (epoch 207):Loss: train=0.004264, val=0.004204;logMAE: train=-5.457444, val=-5.471652\n",
      "2023-09-12 13:29:03 (INFO): 6300/10000 (epoch 211):Loss: train=0.004190, val=0.004204;logMAE: train=-5.475149, val=-5.471662\n",
      "2023-09-12 13:29:10 (INFO): 6400/10000 (epoch 214):Loss: train=0.004223, val=0.004204;logMAE: train=-5.467150, val=-5.471634\n",
      "2023-09-12 13:29:17 (INFO): 6500/10000 (epoch 217):Loss: train=0.004251, val=0.004204;logMAE: train=-5.460500, val=-5.471631\n",
      "2023-09-12 13:29:24 (INFO): 6600/10000 (epoch 221):Loss: train=0.004186, val=0.004204;logMAE: train=-5.475939, val=-5.471638\n",
      "2023-09-12 13:29:31 (INFO): 6700/10000 (epoch 224):Loss: train=0.004254, val=0.004205;logMAE: train=-5.459915, val=-5.471576\n",
      "2023-09-12 13:29:38 (INFO): 6800/10000 (epoch 227):Loss: train=0.004180, val=0.004205;logMAE: train=-5.477330, val=-5.471587\n",
      "2023-09-12 13:29:45 (INFO): 6900/10000 (epoch 231):Loss: train=0.004222, val=0.004205;logMAE: train=-5.467381, val=-5.471598\n",
      "2023-09-12 13:29:53 (INFO): 7000/10000 (epoch 234):Loss: train=0.004201, val=0.004205;logMAE: train=-5.472507, val=-5.471597\n",
      "2023-09-12 13:30:00 (INFO): 7100/10000 (epoch 237):Loss: train=0.004218, val=0.004205;logMAE: train=-5.468297, val=-5.471574\n",
      "2023-09-12 13:30:07 (INFO): 7200/10000 (epoch 241):Loss: train=0.004229, val=0.004204;logMAE: train=-5.465878, val=-5.471614\n",
      "2023-09-12 13:30:14 (INFO): 7300/10000 (epoch 244):Loss: train=0.004231, val=0.004204;logMAE: train=-5.465410, val=-5.471611\n",
      "2023-09-12 13:30:21 (INFO): 7400/10000 (epoch 247):Loss: train=0.004245, val=0.004205;logMAE: train=-5.462008, val=-5.471566\n",
      "2023-09-12 13:30:28 (INFO): 7500/10000 (epoch 251):Loss: train=0.004171, val=0.004204;logMAE: train=-5.479664, val=-5.471639\n",
      "2023-09-12 13:30:35 (INFO): 7600/10000 (epoch 254):Loss: train=0.004180, val=0.004204;logMAE: train=-5.477442, val=-5.471622\n",
      "2023-09-12 13:30:42 (INFO): 7700/10000 (epoch 257):Loss: train=0.004261, val=0.004205;logMAE: train=-5.458183, val=-5.471565\n",
      "2023-09-12 13:30:49 (INFO): 7800/10000 (epoch 261):Loss: train=0.004207, val=0.004205;logMAE: train=-5.470932, val=-5.471564\n",
      "2023-09-12 13:30:56 (INFO): 7900/10000 (epoch 264):Loss: train=0.004225, val=0.004205;logMAE: train=-5.466727, val=-5.471572\n",
      "2023-09-12 13:31:03 (INFO): 8000/10000 (epoch 267):Loss: train=0.004212, val=0.004205;logMAE: train=-5.469800, val=-5.471597\n",
      "2023-09-12 13:31:09 (INFO): 8100/10000 (epoch 271):Loss: train=0.004196, val=0.004205;logMAE: train=-5.473635, val=-5.471590\n",
      "2023-09-12 13:31:16 (INFO): 8200/10000 (epoch 274):Loss: train=0.004181, val=0.004204;logMAE: train=-5.477279, val=-5.471611\n",
      "2023-09-12 13:31:23 (INFO): 8300/10000 (epoch 277):Loss: train=0.004251, val=0.004205;logMAE: train=-5.460578, val=-5.471578\n",
      "2023-09-12 13:31:30 (INFO): 8400/10000 (epoch 281):Loss: train=0.004216, val=0.004204;logMAE: train=-5.468961, val=-5.471620\n",
      "2023-09-12 13:31:37 (INFO): 8500/10000 (epoch 284):Loss: train=0.004216, val=0.004204;logMAE: train=-5.468874, val=-5.471602\n",
      "2023-09-12 13:31:44 (INFO): 8600/10000 (epoch 287):Loss: train=0.004222, val=0.004205;logMAE: train=-5.467409, val=-5.471585\n",
      "2023-09-12 13:31:51 (INFO): 8700/10000 (epoch 291):Loss: train=0.004204, val=0.004204;logMAE: train=-5.471627, val=-5.471614\n",
      "2023-09-12 13:31:58 (INFO): 8800/10000 (epoch 294):Loss: train=0.004213, val=0.004205;logMAE: train=-5.469667, val=-5.471600\n",
      "2023-09-12 13:32:05 (INFO): 8900/10000 (epoch 297):Loss: train=0.004237, val=0.004205;logMAE: train=-5.463855, val=-5.471590\n",
      "2023-09-12 13:32:12 (INFO): 9000/10000 (epoch 301):Loss: train=0.004182, val=0.004204;logMAE: train=-5.476986, val=-5.471623\n",
      "2023-09-12 13:32:19 (INFO): 9100/10000 (epoch 304):Loss: train=0.004223, val=0.004205;logMAE: train=-5.467142, val=-5.471590\n",
      "2023-09-12 13:32:26 (INFO): 9200/10000 (epoch 307):Loss: train=0.004189, val=0.004205;logMAE: train=-5.475370, val=-5.471599\n",
      "2023-09-12 13:32:34 (INFO): 9300/10000 (epoch 311):Loss: train=0.004246, val=0.004204;logMAE: train=-5.461891, val=-5.471608\n",
      "2023-09-12 13:32:41 (INFO): 9400/10000 (epoch 314):Loss: train=0.004204, val=0.004205;logMAE: train=-5.471633, val=-5.471591\n",
      "2023-09-12 13:32:48 (INFO): 9500/10000 (epoch 317):Loss: train=0.004223, val=0.004205;logMAE: train=-5.467105, val=-5.471580\n",
      "2023-09-12 13:32:55 (INFO): 9600/10000 (epoch 321):Loss: train=0.004227, val=0.004205;logMAE: train=-5.466285, val=-5.471600\n",
      "2023-09-12 13:33:02 (INFO): 9700/10000 (epoch 324):Loss: train=0.004204, val=0.004205;logMAE: train=-5.471679, val=-5.471569\n",
      "2023-09-12 13:33:09 (INFO): 9800/10000 (epoch 327):Loss: train=0.004223, val=0.004205;logMAE: train=-5.467104, val=-5.471564\n",
      "2023-09-12 13:33:16 (INFO): 9900/10000 (epoch 331):Loss: train=0.004218, val=0.004205;logMAE: train=-5.468369, val=-5.471565\n",
      "2023-09-12 13:33:23 (INFO): 10000/10000 (epoch 334):Loss: train=0.004189, val=0.004205;logMAE: train=-5.475355, val=-5.471565\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "    else:\n",
    "        step_init = 1\n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # Perform training step\n",
    "        trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "                \n",
    "            epoch = step // steps_per_epoch\n",
    "            logging.info(\n",
    "                f\"{step}/{num_steps} (epoch {epoch + 1}):\"\n",
    "                f\"Loss: train={train['metrics'].loss:.6f}, val={validation['metrics'].loss:.6f};\"\n",
    "                f\"logMAE: train={train['metrics'].mean_log_mae:.6f}, \"\n",
    "                f\"val={validation['metrics'].mean_log_mae:.6f}\"\n",
    "            )\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
