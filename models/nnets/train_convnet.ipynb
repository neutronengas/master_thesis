{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benni/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from convnet.model.convnet import ConvNet\n",
    "from convnet.model.activations import swish\n",
    "from convnet.training.metrics import Metrics\n",
    "from convnet.training.trainer import Trainer\n",
    "from convnet.training.data_container import DataContainer\n",
    "from convnet.training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_corrnet.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "num_basis_fct = config['num_basis_fct']\n",
    "emb_size = config['emb_size']\n",
    "num_interaction_blocks = config['num_interaction_blocks']\n",
    "ao_vals = config['ao_vals']\n",
    "num_grid_points = config['num_grid_points']\n",
    "num_featuers = config['num_features']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "target = config['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create directories***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 22:17:26 (INFO): Directory: ./logging/20230816_221726_corrnet_8mYd9lAU_md_h2.npz_corrs_final\n"
     ]
    }
   ],
   "source": [
    "# Used for creating a random \"unique\" id for this run\n",
    "def id_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# Create directories\n",
    "# A unique directory name is created for this run based on the input\n",
    "if restart is None:\n",
    "    directory = (logdir + \"/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + model_name\n",
    "                 + \"_\" + id_generator()\n",
    "                 + \"_\" + os.path.basename(dataset)\n",
    "                 + \"_\" + '-'.join(target)\n",
    "                 + \"_\" + comment)\n",
    "else:\n",
    "    directory = restart\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "best_dir = os.path.join(directory, 'best')\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, 'logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "best_loss_file = os.path.join(best_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(best_dir, 'ckpt')\n",
    "step_ckpt_folder = log_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create summary writer and metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "train = {}\n",
    "validation = {}\n",
    "train['metrics'] = Metrics('train', target)\n",
    "validation['metrics'] = Metrics('val', target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_container = DataContainer(dataset, target, 0.2)\n",
    "\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size, seed=data_seed, randomized=True)\n",
    "\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"corrnet\":\n",
    "    model = CorrNet(ao_vals=ao_vals, num_featuers=num_featuers, \n",
    "    num_interaction_blocks=num_interaction_blocks, num_grid_points=num_grid_points, activation=swish)\n",
    "else:\n",
    "    model = CorrNet(ao_vals=ao_vals, num_interaction_blocks=num_interaction_blocks, num_grid_points=num_grid_points, activation=swish)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save/load best recorded loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benni/Dokumente/MA/master_thesis/models/densnet/corrnet/training/metrics.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.mean(np.log(self.maes)).item()\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(best_loss_file):\n",
    "    loss_file = np.load(best_loss_file)\n",
    "    metrics_best = {k: v.item() for k, v in loss_file.items()}\n",
    "else:\n",
    "    metrics_best = validation['metrics'].result()\n",
    "    for key in metrics_best.keys():\n",
    "        metrics_best[key] = np.inf\n",
    "    metrics_best['step'] = 0\n",
    "    np.savez(best_loss_file, **metrics_best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize trainer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps, decay_steps, decay_rate, ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set up checkpointing and load latest checkpoint*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"densnet/output/einsum/Einsum:0\", shape=(None, 166, 166), dtype=float32)\n",
      "Tensor(\"densnet/output/einsum/Einsum:0\", shape=(None, 166, 166), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 22:17:45 (INFO): 100/10000 (epoch 4):Loss: train=127615.195312, val=233517.359375;logMAE: train=11.756775, val=12.361012\n",
      "2023-08-16 22:17:57 (INFO): 200/10000 (epoch 8):Loss: train=373.517639, val=204988.796875;logMAE: train=5.922965, val=12.230711\n",
      "2023-08-16 22:18:08 (INFO): 300/10000 (epoch 12):Loss: train=1.650539, val=173024.187500;logMAE: train=0.501102, val=12.061187\n",
      "2023-08-16 22:18:20 (INFO): 400/10000 (epoch 15):Loss: train=0.825439, val=146524.140625;logMAE: train=-0.191840, val=11.894945\n",
      "2023-08-16 22:18:31 (INFO): 500/10000 (epoch 19):Loss: train=0.755327, val=132525.796875;logMAE: train=-0.280605, val=11.794533\n",
      "2023-08-16 22:18:43 (INFO): 600/10000 (epoch 23):Loss: train=0.757188, val=107645.710938;logMAE: train=-0.278144, val=11.586600\n",
      "2023-08-16 22:18:54 (INFO): 700/10000 (epoch 26):Loss: train=0.685637, val=76538.820312;logMAE: train=-0.377407, val=11.245553\n",
      "2023-08-16 22:19:05 (INFO): 800/10000 (epoch 30):Loss: train=0.615744, val=52326.281250;logMAE: train=-0.484925, val=10.865254\n",
      "2023-08-16 22:19:17 (INFO): 900/10000 (epoch 34):Loss: train=0.501844, val=30565.947266;logMAE: train=-0.689466, val=10.327641\n",
      "2023-08-16 22:19:29 (INFO): 1000/10000 (epoch 38):Loss: train=0.466798, val=10930.196289;logMAE: train=-0.761859, val=9.299285\n",
      "2023-08-16 22:19:40 (INFO): 1100/10000 (epoch 41):Loss: train=0.226601, val=1134.461060;logMAE: train=-1.484567, val=7.033913\n",
      "2023-08-16 22:19:52 (INFO): 1200/10000 (epoch 45):Loss: train=0.140593, val=92.353508;logMAE: train=-1.961883, val=4.525624\n",
      "2023-08-16 22:20:03 (INFO): 1300/10000 (epoch 49):Loss: train=0.138070, val=284.758667;logMAE: train=-1.979993, val=5.651642\n",
      "2023-08-16 22:20:15 (INFO): 1400/10000 (epoch 52):Loss: train=0.119602, val=17.092522;logMAE: train=-2.123586, val=2.838641\n",
      "2023-08-16 22:20:28 (INFO): 1500/10000 (epoch 56):Loss: train=0.080024, val=5.609685;logMAE: train=-2.525435, val=1.724495\n",
      "2023-08-16 22:20:42 (INFO): 1600/10000 (epoch 60):Loss: train=0.080683, val=3.012536;logMAE: train=-2.517230, val=1.102782\n",
      "2023-08-16 22:20:55 (INFO): 1700/10000 (epoch 63):Loss: train=0.208744, val=3.047693;logMAE: train=-1.566649, val=1.114385\n",
      "2023-08-16 22:21:05 (INFO): 1800/10000 (epoch 67):Loss: train=0.126365, val=3.003626;logMAE: train=-2.068583, val=1.099820\n",
      "2023-08-16 22:21:14 (INFO): 1900/10000 (epoch 71):Loss: train=0.074228, val=2.924978;logMAE: train=-2.600613, val=1.073287\n",
      "2023-08-16 22:21:24 (INFO): 2000/10000 (epoch 75):Loss: train=0.083974, val=2.723746;logMAE: train=-2.477251, val=1.002008\n",
      "2023-08-16 22:21:34 (INFO): 2100/10000 (epoch 78):Loss: train=0.108505, val=2.397104;logMAE: train=-2.220962, val=0.874261\n",
      "2023-08-16 22:21:44 (INFO): 2200/10000 (epoch 82):Loss: train=0.073778, val=2.012815;logMAE: train=-2.606692, val=0.699534\n",
      "2023-08-16 22:21:54 (INFO): 2300/10000 (epoch 86):Loss: train=0.125802, val=1.726174;logMAE: train=-2.073047, val=0.545907\n",
      "2023-08-16 22:22:03 (INFO): 2400/10000 (epoch 89):Loss: train=0.071337, val=1.521333;logMAE: train=-2.640343, val=0.419587\n",
      "2023-08-16 22:22:12 (INFO): 2500/10000 (epoch 93):Loss: train=0.057372, val=1.374153;logMAE: train=-2.858193, val=0.317838\n",
      "2023-08-16 22:22:22 (INFO): 2600/10000 (epoch 97):Loss: train=0.077257, val=1.287717;logMAE: train=-2.560618, val=0.252871\n",
      "2023-08-16 22:22:31 (INFO): 2700/10000 (epoch 101):Loss: train=0.077088, val=1.233370;logMAE: train=-2.562813, val=0.209750\n",
      "2023-08-16 22:22:42 (INFO): 2800/10000 (epoch 104):Loss: train=0.104582, val=1.177766;logMAE: train=-2.257788, val=0.163620\n",
      "2023-08-16 22:22:51 (INFO): 2900/10000 (epoch 108):Loss: train=0.061478, val=1.135605;logMAE: train=-2.789073, val=0.127166\n",
      "2023-08-16 22:23:02 (INFO): 3000/10000 (epoch 112):Loss: train=0.067442, val=1.083889;logMAE: train=-2.696488, val=0.080555\n",
      "2023-08-16 22:23:12 (INFO): 3100/10000 (epoch 115):Loss: train=0.082382, val=1.015854;logMAE: train=-2.496388, val=0.015729\n",
      "2023-08-16 22:23:23 (INFO): 3200/10000 (epoch 119):Loss: train=0.095734, val=0.938647;logMAE: train=-2.346180, val=-0.063316\n",
      "2023-08-16 22:23:32 (INFO): 3300/10000 (epoch 123):Loss: train=0.086445, val=0.856030;logMAE: train=-2.448249, val=-0.155450\n",
      "2023-08-16 22:23:41 (INFO): 3400/10000 (epoch 126):Loss: train=0.079025, val=0.769955;logMAE: train=-2.537997, val=-0.261423\n",
      "2023-08-16 22:23:50 (INFO): 3500/10000 (epoch 130):Loss: train=0.067472, val=0.685996;logMAE: train=-2.696039, val=-0.376884\n",
      "2023-08-16 22:23:59 (INFO): 3600/10000 (epoch 134):Loss: train=0.079122, val=0.615208;logMAE: train=-2.536761, val=-0.485794\n",
      "2023-08-16 22:24:07 (INFO): 3700/10000 (epoch 138):Loss: train=0.067318, val=0.552584;logMAE: train=-2.698327, val=-0.593150\n",
      "2023-08-16 22:24:16 (INFO): 3800/10000 (epoch 141):Loss: train=0.065770, val=0.494962;logMAE: train=-2.721592, val=-0.703274\n",
      "2023-08-16 22:24:25 (INFO): 3900/10000 (epoch 145):Loss: train=0.072720, val=0.441975;logMAE: train=-2.621144, val=-0.816502\n",
      "2023-08-16 22:24:34 (INFO): 4000/10000 (epoch 149):Loss: train=0.076526, val=0.394205;logMAE: train=-2.570129, val=-0.930885\n",
      "2023-08-16 22:24:42 (INFO): 4100/10000 (epoch 152):Loss: train=0.087623, val=0.352780;logMAE: train=-2.434713, val=-1.041910\n",
      "2023-08-16 22:24:51 (INFO): 4200/10000 (epoch 156):Loss: train=0.074063, val=0.315821;logMAE: train=-2.602845, val=-1.152580\n",
      "2023-08-16 22:25:00 (INFO): 4300/10000 (epoch 160):Loss: train=0.071036, val=0.282917;logMAE: train=-2.644567, val=-1.262602\n",
      "2023-08-16 22:25:08 (INFO): 4400/10000 (epoch 163):Loss: train=0.047729, val=0.252716;logMAE: train=-3.042227, val=-1.375488\n",
      "2023-08-16 22:25:17 (INFO): 4500/10000 (epoch 167):Loss: train=0.057512, val=0.225504;logMAE: train=-2.855764, val=-1.489419\n",
      "2023-08-16 22:25:26 (INFO): 4600/10000 (epoch 171):Loss: train=0.062969, val=0.201240;logMAE: train=-2.765115, val=-1.603258\n",
      "2023-08-16 22:25:38 (INFO): 4700/10000 (epoch 175):Loss: train=0.072883, val=0.180611;logMAE: train=-2.618906, val=-1.711408\n",
      "2023-08-16 22:25:49 (INFO): 4800/10000 (epoch 178):Loss: train=0.061592, val=0.161428;logMAE: train=-2.787222, val=-1.823696\n",
      "2023-08-16 22:26:00 (INFO): 4900/10000 (epoch 182):Loss: train=0.069955, val=0.145196;logMAE: train=-2.659902, val=-1.929669\n",
      "2023-08-16 22:26:11 (INFO): 5000/10000 (epoch 186):Loss: train=0.077901, val=0.129415;logMAE: train=-2.552320, val=-2.044733\n",
      "2023-08-16 22:26:22 (INFO): 5100/10000 (epoch 189):Loss: train=0.039288, val=0.115632;logMAE: train=-3.236836, val=-2.157341\n",
      "2023-08-16 22:26:32 (INFO): 5200/10000 (epoch 193):Loss: train=0.057672, val=0.103696;logMAE: train=-2.852978, val=-2.266289\n",
      "2023-08-16 22:26:43 (INFO): 5300/10000 (epoch 197):Loss: train=0.068101, val=0.093555;logMAE: train=-2.686761, val=-2.369204\n",
      "2023-08-16 22:26:54 (INFO): 5400/10000 (epoch 201):Loss: train=0.037556, val=0.084660;logMAE: train=-3.281915, val=-2.469110\n",
      "2023-08-16 22:27:05 (INFO): 5500/10000 (epoch 204):Loss: train=0.051128, val=0.076337;logMAE: train=-2.973433, val=-2.572599\n",
      "2023-08-16 22:27:16 (INFO): 5600/10000 (epoch 208):Loss: train=0.049070, val=0.069078;logMAE: train=-3.014513, val=-2.672526\n",
      "2023-08-16 22:27:27 (INFO): 5700/10000 (epoch 212):Loss: train=0.037620, val=0.062429;logMAE: train=-3.280207, val=-2.773720\n",
      "2023-08-16 22:27:40 (INFO): 5800/10000 (epoch 215):Loss: train=0.045827, val=0.056622;logMAE: train=-3.082886, val=-2.871359\n",
      "2023-08-16 22:27:52 (INFO): 5900/10000 (epoch 219):Loss: train=0.034431, val=0.051227;logMAE: train=-3.368790, val=-2.971492\n",
      "2023-08-16 22:28:02 (INFO): 6000/10000 (epoch 223):Loss: train=0.059705, val=0.046757;logMAE: train=-2.818342, val=-3.062781\n",
      "2023-08-16 22:28:13 (INFO): 6100/10000 (epoch 226):Loss: train=0.050571, val=0.042719;logMAE: train=-2.984383, val=-3.153106\n",
      "2023-08-16 22:28:24 (INFO): 6200/10000 (epoch 230):Loss: train=0.062327, val=0.038902;logMAE: train=-2.775353, val=-3.246720\n",
      "2023-08-16 22:28:35 (INFO): 6300/10000 (epoch 234):Loss: train=0.046664, val=0.035145;logMAE: train=-3.064792, val=-3.348261\n",
      "2023-08-16 22:28:47 (INFO): 6400/10000 (epoch 238):Loss: train=0.067215, val=0.031719;logMAE: train=-2.699864, val=-3.450826\n",
      "2023-08-16 22:28:59 (INFO): 6500/10000 (epoch 241):Loss: train=0.047323, val=0.029207;logMAE: train=-3.050751, val=-3.533353\n",
      "2023-08-16 22:29:11 (INFO): 6600/10000 (epoch 245):Loss: train=0.048443, val=0.026924;logMAE: train=-3.027370, val=-3.614723\n",
      "2023-08-16 22:29:23 (INFO): 6700/10000 (epoch 249):Loss: train=0.046888, val=0.025162;logMAE: train=-3.059993, val=-3.682400\n",
      "2023-08-16 22:29:35 (INFO): 6800/10000 (epoch 252):Loss: train=0.045148, val=0.023516;logMAE: train=-3.097808, val=-3.750074\n",
      "2023-08-16 22:29:47 (INFO): 6900/10000 (epoch 256):Loss: train=0.032697, val=0.021800;logMAE: train=-3.420460, val=-3.825859\n",
      "2023-08-16 22:29:59 (INFO): 7000/10000 (epoch 260):Loss: train=0.042828, val=0.020179;logMAE: train=-3.150557, val=-3.903099\n",
      "2023-08-16 22:30:11 (INFO): 7100/10000 (epoch 263):Loss: train=0.054236, val=0.018665;logMAE: train=-2.914412, val=-3.981100\n",
      "2023-08-16 22:30:23 (INFO): 7200/10000 (epoch 267):Loss: train=0.045624, val=0.017676;logMAE: train=-3.087317, val=-4.035559\n",
      "2023-08-16 22:30:33 (INFO): 7300/10000 (epoch 271):Loss: train=0.047356, val=0.016753;logMAE: train=-3.050061, val=-4.089186\n",
      "2023-08-16 22:30:44 (INFO): 7400/10000 (epoch 275):Loss: train=0.039341, val=0.015926;logMAE: train=-3.235477, val=-4.139827\n",
      "2023-08-16 22:30:55 (INFO): 7500/10000 (epoch 278):Loss: train=0.041195, val=0.015180;logMAE: train=-3.189438, val=-4.187792\n",
      "2023-08-16 22:31:06 (INFO): 7600/10000 (epoch 282):Loss: train=0.040355, val=0.014574;logMAE: train=-3.210029, val=-4.228518\n",
      "2023-08-16 22:31:20 (INFO): 7700/10000 (epoch 286):Loss: train=0.034546, val=0.013924;logMAE: train=-3.365459, val=-4.274147\n",
      "2023-08-16 22:31:35 (INFO): 7800/10000 (epoch 289):Loss: train=0.036873, val=0.013525;logMAE: train=-3.300287, val=-4.303199\n",
      "2023-08-16 22:31:49 (INFO): 7900/10000 (epoch 293):Loss: train=0.040653, val=0.013216;logMAE: train=-3.202684, val=-4.326356\n",
      "2023-08-16 22:32:00 (INFO): 8000/10000 (epoch 297):Loss: train=0.052164, val=0.012938;logMAE: train=-2.953364, val=-4.347592\n",
      "2023-08-16 22:32:11 (INFO): 8100/10000 (epoch 301):Loss: train=0.037006, val=0.012624;logMAE: train=-3.296668, val=-4.372150\n",
      "2023-08-16 22:32:22 (INFO): 8200/10000 (epoch 304):Loss: train=0.032683, val=0.012351;logMAE: train=-3.420895, val=-4.394048\n",
      "2023-08-16 22:32:33 (INFO): 8300/10000 (epoch 308):Loss: train=0.049521, val=0.012140;logMAE: train=-3.005358, val=-4.411221\n",
      "2023-08-16 22:32:43 (INFO): 8400/10000 (epoch 312):Loss: train=0.026265, val=0.011875;logMAE: train=-3.639524, val=-4.433311\n",
      "2023-08-16 22:32:55 (INFO): 8500/10000 (epoch 315):Loss: train=0.040932, val=0.011724;logMAE: train=-3.195836, val=-4.446074\n",
      "2023-08-16 22:33:07 (INFO): 8600/10000 (epoch 319):Loss: train=0.040334, val=0.011411;logMAE: train=-3.210558, val=-4.473189\n",
      "2023-08-16 22:33:19 (INFO): 8700/10000 (epoch 323):Loss: train=0.026734, val=0.011173;logMAE: train=-3.621808, val=-4.494245\n",
      "2023-08-16 22:33:33 (INFO): 8800/10000 (epoch 326):Loss: train=0.032358, val=0.011001;logMAE: train=-3.430882, val=-4.509787\n",
      "2023-08-16 22:33:50 (INFO): 8900/10000 (epoch 330):Loss: train=0.030530, val=0.010850;logMAE: train=-3.489042, val=-4.523576\n",
      "2023-08-16 22:34:05 (INFO): 9000/10000 (epoch 334):Loss: train=0.035895, val=0.010674;logMAE: train=-3.327161, val=-4.539974\n",
      "2023-08-16 22:34:17 (INFO): 9100/10000 (epoch 338):Loss: train=0.038675, val=0.010532;logMAE: train=-3.252573, val=-4.553321\n",
      "2023-08-16 22:34:28 (INFO): 9200/10000 (epoch 341):Loss: train=0.048242, val=0.010604;logMAE: train=-3.031535, val=-4.546570\n",
      "2023-08-16 22:34:38 (INFO): 9300/10000 (epoch 345):Loss: train=0.038498, val=0.010577;logMAE: train=-3.257140, val=-4.549090\n",
      "2023-08-16 22:34:49 (INFO): 9400/10000 (epoch 349):Loss: train=0.036629, val=0.010432;logMAE: train=-3.306926, val=-4.562919\n",
      "2023-08-16 22:35:00 (INFO): 9500/10000 (epoch 352):Loss: train=0.030503, val=0.010269;logMAE: train=-3.489945, val=-4.578620\n",
      "2023-08-16 22:35:12 (INFO): 9600/10000 (epoch 356):Loss: train=0.054703, val=0.010241;logMAE: train=-2.905831, val=-4.581315\n",
      "2023-08-16 22:35:25 (INFO): 9700/10000 (epoch 360):Loss: train=0.043174, val=0.010262;logMAE: train=-3.142511, val=-4.579309\n",
      "2023-08-16 22:35:40 (INFO): 9800/10000 (epoch 363):Loss: train=0.040987, val=0.010323;logMAE: train=-3.194507, val=-4.573359\n",
      "2023-08-16 22:35:55 (INFO): 9900/10000 (epoch 367):Loss: train=0.046133, val=0.010192;logMAE: train=-3.076232, val=-4.586130\n",
      "2023-08-16 22:36:09 (INFO): 10000/10000 (epoch 371):Loss: train=0.037552, val=0.009961;logMAE: train=-3.282018, val=-4.609075\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "    else:\n",
    "        step_init = 1\n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # Perform training step\n",
    "        trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "                \n",
    "            epoch = step // steps_per_epoch\n",
    "            logging.info(\n",
    "                f\"{step}/{num_steps} (epoch {epoch + 1}):\"\n",
    "                f\"Loss: train={train['metrics'].loss:.6f}, val={validation['metrics'].loss:.6f};\"\n",
    "                f\"logMAE: train={train['metrics'].mean_log_mae:.6f}, \"\n",
    "                f\"val={validation['metrics'].mean_log_mae:.6f}\"\n",
    "            )\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
