{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from model.dmnet import DMNet\n",
    "from model.activations import swish\n",
    "from training.metrics import Metrics\n",
    "from training.trainer import Trainer\n",
    "from training.data_container import DataContainer\n",
    "from training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../configs/config_dmnet.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "emb_size = config['emb_size']\n",
    "num_interaction_blocks = config['num_interaction_blocks']\n",
    "width_ticks = config['width_ticks']\n",
    "length_ticks = config['length_ticks']\n",
    "cutoff = config['cutoff']\n",
    "m_max = config[\"m_max\"]\n",
    "max_no_orbitals_per_m = config[\"max_no_orbitals_per_m\"]\n",
    "max_split_per_m = config[\"max_split_per_m\"]\n",
    "max_number_coeffs_per_ao = config[\"max_number_coeffs_per_ao\"]\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "target = config['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create directories***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 16:14:55 (INFO): Directory: ../logging/20231127_161455_dmnet_LpFUkdHp_md_h2.npz_densities_final\n"
     ]
    }
   ],
   "source": [
    "# Used for creating a random \"unique\" id for this run\n",
    "def id_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# Create directories\n",
    "# A unique directory name is created for this run based on the input\n",
    "if restart is None:\n",
    "    directory = (logdir + \"/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + model_name\n",
    "                 + \"_\" + id_generator()\n",
    "                 + \"_\" + os.path.basename(dataset)\n",
    "                 + \"_\" + '-'.join(target)\n",
    "                 + \"_\" + comment)\n",
    "else:\n",
    "    directory = restart\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "best_dir = os.path.join(directory, 'best')\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, 'logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "best_loss_file = os.path.join(best_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(best_dir, 'ckpt')\n",
    "step_ckpt_folder = log_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create summary writer and metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "train = {}\n",
    "validation = {}\n",
    "train['metrics'] = Metrics('train', target)\n",
    "validation['metrics'] = Metrics('val', target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 16:14:55.798846: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "data_container = DataContainer(dataset, target, cutoff, length_ticks * width_ticks)\n",
    "\n",
    "orbital_parameters = (m_max, max_no_orbitals_per_m, max_split_per_m, max_number_coeffs_per_ao)\n",
    "data_provider = DataProvider(data_container, width_ticks, length_ticks, num_train, num_valid, batch_size, seed=data_seed, randomized=True)\n",
    "\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"dmnet\":\n",
    "    model = DMNet(emb_size, num_interaction_blocks=num_interaction_blocks)\n",
    "else:\n",
    "    model = DMNet(emb_size, num_interaction_blocks=num_interaction_blocks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save/load best recorded loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/Documents/MA/models/nnets/dmnet/training/metrics.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.mean(np.log(self.maes)).item()\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(best_loss_file):\n",
    "    loss_file = np.load(best_loss_file)\n",
    "    metrics_best = {k: v.item() for k, v in loss_file.items()}\n",
    "else:\n",
    "    metrics_best = validation['metrics'].result()\n",
    "    for key in metrics_best.keys():\n",
    "        metrics_best[key] = np.inf\n",
    "    metrics_best['step'] = 0\n",
    "    np.savez(best_loss_file, **metrics_best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize trainer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps, decay_steps, decay_rate, ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set up checkpointing and load latest checkpoint*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dmnet/interaction/mat_weight:0', 'dmnet/interaction/mat_weight:0', 'dmnet/interaction/mat_weight:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dmnet/output/Reshape_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dmnet/output/Reshape_2:0\", shape=(None, 14, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/dmnet/output/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dmnet/interaction/Reshape_2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dmnet/interaction/Reshape_1:0\", shape=(None, 14, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/dmnet/interaction/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dmnet/interaction/Reshape_5:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dmnet/interaction/Reshape_4:0\", shape=(None, 14, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/dmnet/interaction/Cast_1:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dmnet/interaction/Reshape_8:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dmnet/interaction/Reshape_7:0\", shape=(None, 14, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/dmnet/interaction/Cast_2:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2023-11-27 16:14:56 (WARNING): Gradients do not exist for variables ['dmnet/interaction/mat_weight:0', 'dmnet/interaction/mat_weight:0', 'dmnet/interaction/mat_weight:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dmnet/interaction/mat_weight:0', 'dmnet/interaction/mat_weight:0', 'dmnet/interaction/mat_weight:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 16:14:56 (WARNING): Gradients do not exist for variables ['dmnet/interaction/mat_weight:0', 'dmnet/interaction/mat_weight:0', 'dmnet/interaction/mat_weight:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow_addons/optimizers/average_wrapper.py:151: UserWarning: Unable to assign average slot to <tf.Variable 'dmnet/interaction/mat_weight:0' shape=(14, 10, 10) dtype=float32, numpy=\n",
      "array([[[-0.12135544,  0.08238143,  0.06736995, ...,  0.0618072 ,\n",
      "         -0.03225231,  0.08307512],\n",
      "        [-0.02694238, -0.07219773,  0.01484747, ..., -0.13910691,\n",
      "          0.02699903,  0.07334293],\n",
      "        [ 0.0749304 , -0.02807275, -0.06178413, ..., -0.06371115,\n",
      "         -0.04630394,  0.04669525],\n",
      "        ...,\n",
      "        [ 0.13659728, -0.01948842, -0.00569308, ...,  0.0786279 ,\n",
      "          0.07896233, -0.10902683],\n",
      "        [-0.04759008, -0.11427416,  0.03499538, ...,  0.10283874,\n",
      "         -0.07646321,  0.024652  ],\n",
      "        [-0.03279715,  0.13344261, -0.09645361, ...,  0.01434037,\n",
      "         -0.13725734, -0.05068446]],\n",
      "\n",
      "       [[ 0.09206486,  0.02396338,  0.08235092, ...,  0.12193865,\n",
      "         -0.02359302, -0.06230367],\n",
      "        [ 0.10593227,  0.05434832, -0.13070858, ...,  0.11464784,\n",
      "         -0.00925806, -0.1250539 ],\n",
      "        [ 0.01364106, -0.1456833 ,  0.01854478, ..., -0.11301263,\n",
      "          0.12241158,  0.05310312],\n",
      "        ...,\n",
      "        [-0.0471394 , -0.11313663,  0.02168526, ..., -0.14271288,\n",
      "          0.06882633,  0.05120948],\n",
      "        [ 0.09589237, -0.07034815,  0.05466054, ..., -0.07340603,\n",
      "          0.10375461,  0.00182503],\n",
      "        [ 0.09142299,  0.01540449,  0.08520603, ...,  0.11784956,\n",
      "          0.0076834 , -0.0777758 ]],\n",
      "\n",
      "       [[ 0.0427257 ,  0.11448294, -0.12849976, ..., -0.01746295,\n",
      "         -0.13065137, -0.12163423],\n",
      "        [ 0.00078167, -0.08251583, -0.00100842, ..., -0.12069962,\n",
      "         -0.14069302,  0.03573637],\n",
      "        [-0.13641424, -0.0465168 , -0.11554441, ...,  0.06074245,\n",
      "          0.13537353,  0.02808109],\n",
      "        ...,\n",
      "        [ 0.1276227 ,  0.06195913,  0.14556861, ..., -0.02112371,\n",
      "          0.13301396, -0.14421606],\n",
      "        [-0.09702442, -0.05563306, -0.04936594, ..., -0.0023282 ,\n",
      "          0.04184982, -0.13246775],\n",
      "        [-0.13764252, -0.12738049, -0.11787907, ...,  0.13988343,\n",
      "         -0.09242123, -0.0404921 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.09534097,  0.12451333,  0.02386251, ..., -0.02626003,\n",
      "          0.09224994, -0.07918974],\n",
      "        [ 0.06717451,  0.04023461,  0.06890947, ...,  0.14363289,\n",
      "         -0.06628921, -0.09859496],\n",
      "        [-0.06409768,  0.11689919, -0.00531304, ...,  0.01458614,\n",
      "         -0.07882255,  0.14365995],\n",
      "        ...,\n",
      "        [-0.03294195, -0.09470654,  0.07581343, ..., -0.09768324,\n",
      "         -0.1294118 ,  0.10435006],\n",
      "        [ 0.04465532,  0.12510622, -0.11776693, ...,  0.08403444,\n",
      "         -0.0757817 ,  0.09764802],\n",
      "        [ 0.06273903, -0.13082765,  0.00359967, ..., -0.01008298,\n",
      "          0.0878799 ,  0.05223109]],\n",
      "\n",
      "       [[-0.03263661,  0.12779713,  0.11310211, ...,  0.09812512,\n",
      "         -0.08603646,  0.0514729 ],\n",
      "        [-0.02184043, -0.00062253,  0.0416    , ..., -0.01925291,\n",
      "         -0.02858325,  0.11742043],\n",
      "        [-0.05111629,  0.10152908, -0.10341358, ...,  0.02122203,\n",
      "         -0.12207461,  0.0048714 ],\n",
      "        ...,\n",
      "        [ 0.0326758 ,  0.07936431, -0.02511134, ...,  0.10487512,\n",
      "         -0.05191921, -0.02662531],\n",
      "        [-0.13717501, -0.01148278,  0.12942806, ..., -0.11414854,\n",
      "         -0.04018089,  0.10205401],\n",
      "        [ 0.08466324, -0.07006933, -0.03776069, ...,  0.02924584,\n",
      "         -0.01777734, -0.0183553 ]],\n",
      "\n",
      "       [[-0.0120161 , -0.06329737, -0.11818072, ..., -0.01002249,\n",
      "          0.05897859, -0.13671198],\n",
      "        [ 0.01240183,  0.04883827, -0.0584089 , ...,  0.14029467,\n",
      "          0.07609601, -0.03277523],\n",
      "        [ 0.03750986,  0.10082781,  0.1405828 , ..., -0.12607087,\n",
      "         -0.06246997, -0.04607234],\n",
      "        ...,\n",
      "        [-0.14509581, -0.08050816,  0.13305598, ...,  0.03938596,\n",
      "          0.1239357 ,  0.1369785 ],\n",
      "        [-0.00034127,  0.02390245,  0.10574088, ..., -0.06770808,\n",
      "         -0.08606738, -0.10124676],\n",
      "        [ 0.0978982 ,  0.12676385, -0.12268443, ...,  0.05299522,\n",
      "         -0.13031723, -0.10150003]]], dtype=float32)> : 'dmnet/interaction/mat_weight_245'\n",
      "  warnings.warn(\"Unable to assign average slot to {} : {}\".format(var, e))\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow_addons/optimizers/average_wrapper.py:151: UserWarning: Unable to assign average slot to <tf.Variable 'dmnet/interaction/mat_weight:0' shape=(14, 10, 10) dtype=float32, numpy=\n",
      "array([[[ 0.01864435, -0.14595158, -0.14258155, ...,  0.01726171,\n",
      "          0.11003903, -0.08621909],\n",
      "        [ 0.13752109, -0.05725773, -0.06463222, ...,  0.08783445,\n",
      "          0.12048155, -0.08145526],\n",
      "        [-0.06509043, -0.11919829, -0.03268152, ..., -0.0692516 ,\n",
      "          0.11095089, -0.00835137],\n",
      "        ...,\n",
      "        [-0.13186285,  0.11904636,  0.03761595, ...,  0.02959251,\n",
      "          0.11186984, -0.04658482],\n",
      "        [-0.02344482, -0.05257741, -0.03108565, ...,  0.03165522,\n",
      "          0.04299346,  0.00202477],\n",
      "        [ 0.08265506, -0.0884151 ,  0.07670225, ..., -0.09698749,\n",
      "         -0.06340521,  0.09942518]],\n",
      "\n",
      "       [[ 0.10408071,  0.02080663, -0.13016911, ...,  0.13109899,\n",
      "         -0.12107073, -0.02221432],\n",
      "        [ 0.05437505, -0.00267813, -0.02082747, ..., -0.01582661,\n",
      "          0.06117006, -0.00526389],\n",
      "        [ 0.13147306, -0.13910046, -0.00749971, ..., -0.10107271,\n",
      "          0.05419783, -0.05672633],\n",
      "        ...,\n",
      "        [-0.04699976, -0.03288262, -0.02846619, ...,  0.14467126,\n",
      "         -0.06572654, -0.12951356],\n",
      "        [-0.07173176,  0.01058409,  0.13373092, ..., -0.14422922,\n",
      "          0.08155194,  0.0182246 ],\n",
      "        [-0.09879044,  0.08521038, -0.08823662, ...,  0.06794232,\n",
      "          0.11906779, -0.03916873]],\n",
      "\n",
      "       [[ 0.07909425,  0.0251106 ,  0.12419397, ..., -0.07604569,\n",
      "          0.02575229,  0.00247549],\n",
      "        [ 0.10568362, -0.14377223,  0.02249803, ...,  0.06710021,\n",
      "         -0.09304537, -0.11981377],\n",
      "        [-0.1142239 , -0.11491627, -0.03938854, ..., -0.12363531,\n",
      "          0.12863645,  0.09989987],\n",
      "        ...,\n",
      "        [ 0.09037273,  0.10412392,  0.09169945, ..., -0.12518547,\n",
      "          0.10358338,  0.12911397],\n",
      "        [ 0.14073539, -0.08461582, -0.07076396, ..., -0.08206833,\n",
      "         -0.10929129,  0.1292386 ],\n",
      "        [ 0.08962794, -0.10434712, -0.11162329, ...,  0.13437918,\n",
      "         -0.05490638, -0.03174105]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.04905134,  0.02146535, -0.03609113, ..., -0.13114952,\n",
      "         -0.01224324, -0.02750701],\n",
      "        [-0.10547665,  0.01688272, -0.03744689, ...,  0.11521101,\n",
      "          0.05986208, -0.06585592],\n",
      "        [ 0.12497446, -0.10285856,  0.14081797, ..., -0.03792465,\n",
      "         -0.12043137,  0.11557314],\n",
      "        ...,\n",
      "        [ 0.08893313, -0.04790401, -0.11215176, ...,  0.04839206,\n",
      "          0.04278949, -0.08681828],\n",
      "        [-0.04940513,  0.13668117,  0.11405429, ..., -0.02948568,\n",
      "         -0.12640885,  0.11638513],\n",
      "        [-0.00714855, -0.05699597,  0.00796515, ...,  0.11653808,\n",
      "          0.06427529,  0.0532057 ]],\n",
      "\n",
      "       [[ 0.10702935,  0.0115678 , -0.01286897, ..., -0.08908401,\n",
      "          0.05836071,  0.08689949],\n",
      "        [ 0.0802391 ,  0.13142374, -0.08808585, ...,  0.06552573,\n",
      "          0.05362342, -0.11655574],\n",
      "        [-0.00542022,  0.09963326, -0.11742253, ..., -0.05218279,\n",
      "          0.01866341,  0.01726884],\n",
      "        ...,\n",
      "        [-0.00415269, -0.04831601, -0.08519761, ...,  0.12261111,\n",
      "          0.07313678, -0.13926686],\n",
      "        [-0.02134633, -0.04850817, -0.13851702, ...,  0.08202858,\n",
      "          0.10083367, -0.0837888 ],\n",
      "        [ 0.13566521, -0.10436743, -0.02152276, ...,  0.10452008,\n",
      "          0.10046659,  0.12481052]],\n",
      "\n",
      "       [[-0.01190177,  0.12099668, -0.00541009, ..., -0.05388948,\n",
      "          0.10001853, -0.0488878 ],\n",
      "        [ 0.1015542 , -0.00441933,  0.07226616, ..., -0.10478362,\n",
      "         -0.04584946, -0.08664035],\n",
      "        [-0.02467351,  0.07509314, -0.03891842, ..., -0.09821356,\n",
      "         -0.09077828,  0.09764077],\n",
      "        ...,\n",
      "        [ 0.13345799, -0.11122427,  0.11183909, ...,  0.11393294,\n",
      "         -0.11944515,  0.06254637],\n",
      "        [ 0.13079321, -0.00603203,  0.04315972, ..., -0.07877567,\n",
      "          0.00015123,  0.12694043],\n",
      "        [ 0.06686522, -0.07582033, -0.01498139, ..., -0.02949032,\n",
      "          0.10746104, -0.10359542]]], dtype=float32)> : 'dmnet/interaction/mat_weight_393'\n",
      "  warnings.warn(\"Unable to assign average slot to {} : {}\".format(var, e))\n",
      "/Users/benni/anaconda3/envs/tf_old/lib/python3.10/site-packages/tensorflow_addons/optimizers/average_wrapper.py:151: UserWarning: Unable to assign average slot to <tf.Variable 'dmnet/interaction/mat_weight:0' shape=(14, 10, 10) dtype=float32, numpy=\n",
      "array([[[-0.02474167,  0.04888901, -0.11394005, ...,  0.10003403,\n",
      "          0.05754431,  0.11223567],\n",
      "        [-0.04541791, -0.05961654,  0.10818806, ...,  0.04242174,\n",
      "          0.14494523,  0.13021496],\n",
      "        [ 0.01980536, -0.04837723, -0.03004179, ...,  0.00922249,\n",
      "          0.08555368,  0.02760595],\n",
      "        ...,\n",
      "        [-0.00466123,  0.00561734,  0.05329278, ..., -0.12987995,\n",
      "          0.14502835, -0.06654486],\n",
      "        [-0.05736037, -0.04547697,  0.105528  , ...,  0.05612858,\n",
      "         -0.14303857,  0.11885604],\n",
      "        [-0.05234253,  0.07055543,  0.04412664, ...,  0.00425376,\n",
      "          0.07126138, -0.11580257]],\n",
      "\n",
      "       [[-0.10519964, -0.07617611,  0.00435148, ..., -0.14145222,\n",
      "          0.14126244, -0.01317565],\n",
      "        [-0.13842018, -0.02226298,  0.06986408, ..., -0.05757003,\n",
      "         -0.09845944, -0.1449441 ],\n",
      "        [-0.06913263, -0.02980712, -0.04834543, ...,  0.01704568,\n",
      "         -0.01447731, -0.03556378],\n",
      "        ...,\n",
      "        [ 0.09200884,  0.01168598,  0.07847612, ...,  0.04478635,\n",
      "          0.14224443, -0.13693668],\n",
      "        [-0.05862508, -0.0145763 , -0.02586699, ..., -0.04062685,\n",
      "         -0.02503788, -0.13764265],\n",
      "        [-0.1284114 , -0.09456973, -0.01201279, ...,  0.06665057,\n",
      "          0.09950095,  0.04281333]],\n",
      "\n",
      "       [[-0.10942632,  0.0262747 , -0.10276513, ..., -0.08601475,\n",
      "         -0.01703338,  0.01278786],\n",
      "        [-0.0107328 ,  0.05907743, -0.05825855, ...,  0.07100743,\n",
      "          0.03414771,  0.05647109],\n",
      "        [ 0.05866636,  0.12353832,  0.02776043, ...,  0.00849219,\n",
      "          0.05792029, -0.08544175],\n",
      "        ...,\n",
      "        [ 0.11085516, -0.05688014, -0.01405738, ...,  0.1025538 ,\n",
      "          0.01080246,  0.10044616],\n",
      "        [ 0.08562313, -0.13781126,  0.03105412, ..., -0.04847798,\n",
      "         -0.09275886, -0.10026036],\n",
      "        [ 0.06914553,  0.10831407, -0.09051742, ..., -0.03146233,\n",
      "          0.1448456 ,  0.08400153]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.09979185,  0.02727583,  0.00173332, ..., -0.05588327,\n",
      "         -0.08623438, -0.00264134],\n",
      "        [-0.04977264, -0.08695596,  0.06617707, ...,  0.08361053,\n",
      "          0.10337098,  0.06256281],\n",
      "        [-0.07790881, -0.12505892, -0.03272285, ..., -0.00632502,\n",
      "          0.10338664,  0.0989335 ],\n",
      "        ...,\n",
      "        [ 0.01840122, -0.07023016, -0.13376924, ...,  0.08242998,\n",
      "          0.01495229, -0.07827547],\n",
      "        [ 0.10622984, -0.00238369, -0.04908928, ...,  0.05185036,\n",
      "         -0.01110913,  0.10755175],\n",
      "        [-0.02444791,  0.08954923, -0.00801939, ...,  0.07461336,\n",
      "          0.09229782, -0.01943772]],\n",
      "\n",
      "       [[ 0.02832721,  0.0556228 ,  0.11978346, ..., -0.09754068,\n",
      "          0.11648223,  0.0602501 ],\n",
      "        [ 0.08462249, -0.04209562,  0.05657601, ..., -0.09836286,\n",
      "          0.03136422,  0.10110605],\n",
      "        [ 0.14112705, -0.08613785,  0.13136458, ...,  0.12970793,\n",
      "          0.09164497,  0.01759955],\n",
      "        ...,\n",
      "        [-0.14628838,  0.12175125, -0.14476031, ...,  0.11686975,\n",
      "          0.10355808,  0.1259011 ],\n",
      "        [ 0.0545117 , -0.04610047,  0.13220364, ...,  0.09114358,\n",
      "         -0.10423718, -0.01742937],\n",
      "        [ 0.12264726, -0.10018344,  0.07747573, ...,  0.0259832 ,\n",
      "         -0.03911589, -0.04978154]],\n",
      "\n",
      "       [[-0.05890851,  0.07627359,  0.06393829, ..., -0.01994497,\n",
      "         -0.08375851, -0.12287308],\n",
      "        [-0.0425203 , -0.0651142 ,  0.01614369, ...,  0.13526437,\n",
      "         -0.11744037, -0.13987254],\n",
      "        [ 0.02308056, -0.09561117, -0.13704142, ...,  0.05648471,\n",
      "         -0.03511949,  0.0834904 ],\n",
      "        ...,\n",
      "        [ 0.12355831, -0.14298594,  0.0583414 , ...,  0.09081136,\n",
      "          0.04333681, -0.09396158],\n",
      "        [-0.09182882,  0.04890922, -0.03481878, ..., -0.08205155,\n",
      "          0.00860789,  0.08929262],\n",
      "        [-0.09947317,  0.0368243 , -0.08969739, ..., -0.01396616,\n",
      "          0.07528454, -0.09225996]]], dtype=float32)> : 'dmnet/interaction/mat_weight_541'\n",
      "  warnings.warn(\"Unable to assign average slot to {} : {}\".format(var, e))\n",
      "2023-11-27 16:15:01 (INFO): 100/10000 (epoch 4):Loss: train=0.092393, val=0.095490;logMAE: train=-2.381701, val=-2.348734\n",
      "2023-11-27 16:15:06 (INFO): 200/10000 (epoch 8):Loss: train=0.073439, val=0.093230;logMAE: train=-2.611302, val=-2.372688\n",
      "2023-11-27 16:15:11 (INFO): 300/10000 (epoch 12):Loss: train=0.048601, val=0.088511;logMAE: train=-3.024109, val=-2.424629\n",
      "2023-11-27 16:15:16 (INFO): 400/10000 (epoch 15):Loss: train=0.023963, val=0.081378;logMAE: train=-3.731233, val=-2.508646\n",
      "2023-11-27 16:15:21 (INFO): 500/10000 (epoch 19):Loss: train=0.007271, val=0.072737;logMAE: train=-4.923930, val=-2.620904\n",
      "2023-11-27 16:15:26 (INFO): 600/10000 (epoch 23):Loss: train=0.002089, val=0.064256;logMAE: train=-6.171251, val=-2.744874\n",
      "2023-11-27 16:15:30 (INFO): 700/10000 (epoch 26):Loss: train=0.000680, val=0.056407;logMAE: train=-7.292942, val=-2.875166\n",
      "2023-11-27 16:15:35 (INFO): 800/10000 (epoch 30):Loss: train=0.000356, val=0.049386;logMAE: train=-7.940050, val=-3.008081\n",
      "2023-11-27 16:15:40 (INFO): 900/10000 (epoch 34):Loss: train=0.000259, val=0.043529;logMAE: train=-8.258707, val=-3.134327\n",
      "2023-11-27 16:15:45 (INFO): 1000/10000 (epoch 38):Loss: train=0.000206, val=0.038347;logMAE: train=-8.486962, val=-3.261067\n",
      "2023-11-27 16:15:50 (INFO): 1100/10000 (epoch 41):Loss: train=0.000179, val=0.033765;logMAE: train=-8.625360, val=-3.388324\n",
      "2023-11-27 16:15:55 (INFO): 1200/10000 (epoch 45):Loss: train=0.000170, val=0.029694;logMAE: train=-8.679787, val=-3.516812\n",
      "2023-11-27 16:16:00 (INFO): 1300/10000 (epoch 49):Loss: train=0.000165, val=0.026063;logMAE: train=-8.707767, val=-3.647246\n",
      "2023-11-27 16:16:05 (INFO): 1400/10000 (epoch 52):Loss: train=0.000162, val=0.023021;logMAE: train=-8.728994, val=-3.771358\n",
      "2023-11-27 16:16:10 (INFO): 1500/10000 (epoch 56):Loss: train=0.000159, val=0.020351;logMAE: train=-8.746750, val=-3.894644\n",
      "2023-11-27 16:16:15 (INFO): 1600/10000 (epoch 60):Loss: train=0.000156, val=0.017972;logMAE: train=-8.762987, val=-4.018932\n",
      "2023-11-27 16:16:20 (INFO): 1700/10000 (epoch 63):Loss: train=0.000154, val=0.015851;logMAE: train=-8.780085, val=-4.144548\n",
      "2023-11-27 16:16:24 (INFO): 1800/10000 (epoch 67):Loss: train=0.000152, val=0.013969;logMAE: train=-8.792356, val=-4.270944\n",
      "2023-11-27 16:16:29 (INFO): 1900/10000 (epoch 71):Loss: train=0.000150, val=0.012394;logMAE: train=-8.802110, val=-4.390568\n",
      "2023-11-27 16:16:34 (INFO): 2000/10000 (epoch 75):Loss: train=0.000149, val=0.010982;logMAE: train=-8.809942, val=-4.511508\n",
      "2023-11-27 16:16:39 (INFO): 2100/10000 (epoch 78):Loss: train=0.000148, val=0.009740;logMAE: train=-8.820241, val=-4.631483\n",
      "2023-11-27 16:16:44 (INFO): 2200/10000 (epoch 82):Loss: train=0.000147, val=0.008693;logMAE: train=-8.825883, val=-4.745183\n",
      "2023-11-27 16:16:48 (INFO): 2300/10000 (epoch 86):Loss: train=0.000146, val=0.007751;logMAE: train=-8.835248, val=-4.859937\n",
      "2023-11-27 16:16:53 (INFO): 2400/10000 (epoch 89):Loss: train=0.000144, val=0.006902;logMAE: train=-8.844474, val=-4.975884\n",
      "2023-11-27 16:16:58 (INFO): 2500/10000 (epoch 93):Loss: train=0.000143, val=0.006138;logMAE: train=-8.850085, val=-5.093201\n",
      "2023-11-27 16:17:02 (INFO): 2600/10000 (epoch 97):Loss: train=0.000142, val=0.005450;logMAE: train=-8.856282, val=-5.212074\n",
      "2023-11-27 16:17:07 (INFO): 2700/10000 (epoch 101):Loss: train=0.000142, val=0.004830;logMAE: train=-8.858016, val=-5.332815\n",
      "2023-11-27 16:17:12 (INFO): 2800/10000 (epoch 104):Loss: train=0.000141, val=0.004272;logMAE: train=-8.863884, val=-5.455630\n",
      "2023-11-27 16:17:17 (INFO): 2900/10000 (epoch 108):Loss: train=0.000141, val=0.003769;logMAE: train=-8.868520, val=-5.580814\n",
      "2023-11-27 16:17:21 (INFO): 3000/10000 (epoch 112):Loss: train=0.000140, val=0.003317;logMAE: train=-8.875802, val=-5.708834\n",
      "2023-11-27 16:17:26 (INFO): 3100/10000 (epoch 115):Loss: train=0.000138, val=0.002912;logMAE: train=-8.885017, val=-5.838750\n",
      "2023-11-27 16:17:31 (INFO): 3200/10000 (epoch 119):Loss: train=0.000138, val=0.002553;logMAE: train=-8.891320, val=-5.970458\n",
      "2023-11-27 16:17:36 (INFO): 3300/10000 (epoch 123):Loss: train=0.000136, val=0.002253;logMAE: train=-8.899652, val=-6.095323\n",
      "2023-11-27 16:17:41 (INFO): 3400/10000 (epoch 126):Loss: train=0.000136, val=0.002003;logMAE: train=-8.906205, val=-6.213076\n",
      "2023-11-27 16:17:46 (INFO): 3500/10000 (epoch 130):Loss: train=0.000137, val=0.001782;logMAE: train=-8.896082, val=-6.329939\n",
      "2023-11-27 16:17:50 (INFO): 3600/10000 (epoch 134):Loss: train=0.000134, val=0.001586;logMAE: train=-8.915514, val=-6.446293\n",
      "2023-11-27 16:17:55 (INFO): 3700/10000 (epoch 138):Loss: train=0.000134, val=0.001414;logMAE: train=-8.917608, val=-6.560996\n",
      "2023-11-27 16:18:00 (INFO): 3800/10000 (epoch 141):Loss: train=0.000134, val=0.001266;logMAE: train=-8.921275, val=-6.671768\n",
      "2023-11-27 16:18:05 (INFO): 3900/10000 (epoch 145):Loss: train=0.000133, val=0.001098;logMAE: train=-8.928710, val=-6.814122\n",
      "2023-11-27 16:18:10 (INFO): 4000/10000 (epoch 149):Loss: train=0.000129, val=0.000945;logMAE: train=-8.954856, val=-6.964440\n",
      "2023-11-27 16:18:14 (INFO): 4100/10000 (epoch 152):Loss: train=0.000128, val=0.000810;logMAE: train=-8.961795, val=-7.118526\n",
      "2023-11-27 16:18:19 (INFO): 4200/10000 (epoch 156):Loss: train=0.000128, val=0.000690;logMAE: train=-8.966263, val=-7.278104\n",
      "2023-11-27 16:18:24 (INFO): 4300/10000 (epoch 160):Loss: train=0.000127, val=0.000585;logMAE: train=-8.970917, val=-7.443937\n",
      "2023-11-27 16:18:29 (INFO): 4400/10000 (epoch 163):Loss: train=0.000126, val=0.000492;logMAE: train=-8.975800, val=-7.616642\n",
      "2023-11-27 16:18:34 (INFO): 4500/10000 (epoch 167):Loss: train=0.000126, val=0.000413;logMAE: train=-8.979253, val=-7.793067\n",
      "2023-11-27 16:18:39 (INFO): 4600/10000 (epoch 171):Loss: train=0.000125, val=0.000345;logMAE: train=-8.983541, val=-7.972837\n",
      "2023-11-27 16:18:44 (INFO): 4700/10000 (epoch 175):Loss: train=0.000125, val=0.000297;logMAE: train=-8.986160, val=-8.121990\n",
      "2023-11-27 16:18:49 (INFO): 4800/10000 (epoch 178):Loss: train=0.000125, val=0.000260;logMAE: train=-8.990343, val=-8.252948\n",
      "2023-11-27 16:18:53 (INFO): 4900/10000 (epoch 182):Loss: train=0.000124, val=0.000231;logMAE: train=-8.993775, val=-8.372744\n",
      "2023-11-27 16:18:58 (INFO): 5000/10000 (epoch 186):Loss: train=0.000124, val=0.000208;logMAE: train=-8.993688, val=-8.477854\n",
      "2023-11-27 16:19:03 (INFO): 5100/10000 (epoch 189):Loss: train=0.000123, val=0.000190;logMAE: train=-9.002240, val=-8.569849\n",
      "2023-11-27 16:19:08 (INFO): 5200/10000 (epoch 193):Loss: train=0.000124, val=0.000175;logMAE: train=-8.997786, val=-8.650765\n",
      "2023-11-27 16:19:12 (INFO): 5300/10000 (epoch 197):Loss: train=0.000123, val=0.000163;logMAE: train=-9.005761, val=-8.718848\n",
      "2023-11-27 16:19:17 (INFO): 5400/10000 (epoch 201):Loss: train=0.000123, val=0.000155;logMAE: train=-9.006930, val=-8.771814\n",
      "2023-11-27 16:19:22 (INFO): 5500/10000 (epoch 204):Loss: train=0.000122, val=0.000149;logMAE: train=-9.014372, val=-8.814694\n",
      "2023-11-27 16:19:27 (INFO): 5600/10000 (epoch 208):Loss: train=0.000122, val=0.000143;logMAE: train=-9.009651, val=-8.849766\n",
      "2023-11-27 16:19:31 (INFO): 5700/10000 (epoch 212):Loss: train=0.000121, val=0.000139;logMAE: train=-9.018245, val=-8.878141\n",
      "2023-11-27 16:19:36 (INFO): 5800/10000 (epoch 215):Loss: train=0.000121, val=0.000136;logMAE: train=-9.021786, val=-8.902353\n",
      "2023-11-27 16:19:41 (INFO): 5900/10000 (epoch 219):Loss: train=0.000120, val=0.000133;logMAE: train=-9.025244, val=-8.922978\n",
      "2023-11-27 16:19:46 (INFO): 6000/10000 (epoch 223):Loss: train=0.000120, val=0.000131;logMAE: train=-9.026737, val=-8.939988\n",
      "2023-11-27 16:19:50 (INFO): 6100/10000 (epoch 226):Loss: train=0.000120, val=0.000129;logMAE: train=-9.029223, val=-8.953945\n",
      "2023-11-27 16:19:55 (INFO): 6200/10000 (epoch 230):Loss: train=0.000120, val=0.000128;logMAE: train=-9.029389, val=-8.965961\n",
      "2023-11-27 16:20:00 (INFO): 6300/10000 (epoch 234):Loss: train=0.000119, val=0.000126;logMAE: train=-9.032280, val=-8.976434\n",
      "2023-11-27 16:20:04 (INFO): 6400/10000 (epoch 238):Loss: train=0.000119, val=0.000125;logMAE: train=-9.032449, val=-8.985227\n",
      "2023-11-27 16:20:09 (INFO): 6500/10000 (epoch 241):Loss: train=0.000119, val=0.000124;logMAE: train=-9.040049, val=-8.992504\n",
      "2023-11-27 16:20:14 (INFO): 6600/10000 (epoch 245):Loss: train=0.000118, val=0.000124;logMAE: train=-9.044168, val=-8.998508\n",
      "2023-11-27 16:20:19 (INFO): 6700/10000 (epoch 249):Loss: train=0.000118, val=0.000123;logMAE: train=-9.043521, val=-9.003175\n",
      "2023-11-27 16:20:24 (INFO): 6800/10000 (epoch 252):Loss: train=0.000118, val=0.000123;logMAE: train=-9.043451, val=-9.006942\n",
      "2023-11-27 16:20:29 (INFO): 6900/10000 (epoch 256):Loss: train=0.000118, val=0.000122;logMAE: train=-9.046664, val=-9.010189\n",
      "2023-11-27 16:20:34 (INFO): 7000/10000 (epoch 260):Loss: train=0.000117, val=0.000122;logMAE: train=-9.049769, val=-9.013023\n",
      "2023-11-27 16:20:39 (INFO): 7100/10000 (epoch 263):Loss: train=0.000117, val=0.000122;logMAE: train=-9.053596, val=-9.015580\n",
      "2023-11-27 16:20:43 (INFO): 7200/10000 (epoch 267):Loss: train=0.000117, val=0.000121;logMAE: train=-9.052778, val=-9.017786\n",
      "2023-11-27 16:20:48 (INFO): 7300/10000 (epoch 271):Loss: train=0.000117, val=0.000121;logMAE: train=-9.053236, val=-9.019828\n",
      "2023-11-27 16:20:53 (INFO): 7400/10000 (epoch 275):Loss: train=0.000116, val=0.000121;logMAE: train=-9.059164, val=-9.021717\n",
      "2023-11-27 16:20:58 (INFO): 7500/10000 (epoch 278):Loss: train=0.000116, val=0.000121;logMAE: train=-9.060500, val=-9.023521\n",
      "2023-11-27 16:21:02 (INFO): 7600/10000 (epoch 282):Loss: train=0.000116, val=0.000120;logMAE: train=-9.062976, val=-9.025274\n",
      "2023-11-27 16:21:07 (INFO): 7700/10000 (epoch 286):Loss: train=0.000116, val=0.000120;logMAE: train=-9.063844, val=-9.027010\n",
      "2023-11-27 16:21:12 (INFO): 7800/10000 (epoch 289):Loss: train=0.000115, val=0.000120;logMAE: train=-9.066399, val=-9.028746\n",
      "2023-11-27 16:21:17 (INFO): 7900/10000 (epoch 293):Loss: train=0.000116, val=0.000120;logMAE: train=-9.065534, val=-9.030419\n",
      "2023-11-27 16:21:21 (INFO): 8000/10000 (epoch 297):Loss: train=0.000115, val=0.000120;logMAE: train=-9.069280, val=-9.032159\n",
      "2023-11-27 16:21:26 (INFO): 8100/10000 (epoch 301):Loss: train=0.000115, val=0.000119;logMAE: train=-9.070966, val=-9.033937\n",
      "2023-11-27 16:21:31 (INFO): 8200/10000 (epoch 304):Loss: train=0.000115, val=0.000119;logMAE: train=-9.072030, val=-9.035667\n",
      "2023-11-27 16:21:35 (INFO): 8300/10000 (epoch 308):Loss: train=0.000114, val=0.000119;logMAE: train=-9.075826, val=-9.037407\n",
      "2023-11-27 16:21:40 (INFO): 8400/10000 (epoch 312):Loss: train=0.000115, val=0.000119;logMAE: train=-9.074576, val=-9.039200\n",
      "2023-11-27 16:21:45 (INFO): 8500/10000 (epoch 315):Loss: train=0.000114, val=0.000118;logMAE: train=-9.077596, val=-9.041009\n",
      "2023-11-27 16:21:50 (INFO): 8600/10000 (epoch 319):Loss: train=0.000114, val=0.000118;logMAE: train=-9.079907, val=-9.042841\n",
      "2023-11-27 16:21:54 (INFO): 8700/10000 (epoch 323):Loss: train=0.000114, val=0.000118;logMAE: train=-9.080023, val=-9.044677\n",
      "2023-11-27 16:21:59 (INFO): 8800/10000 (epoch 326):Loss: train=0.000114, val=0.000118;logMAE: train=-9.082417, val=-9.046556\n",
      "2023-11-27 16:22:04 (INFO): 8900/10000 (epoch 330):Loss: train=0.000113, val=0.000118;logMAE: train=-9.084069, val=-9.048389\n",
      "2023-11-27 16:22:08 (INFO): 9000/10000 (epoch 334):Loss: train=0.000113, val=0.000117;logMAE: train=-9.086442, val=-9.050246\n",
      "2023-11-27 16:22:13 (INFO): 9100/10000 (epoch 338):Loss: train=0.000113, val=0.000117;logMAE: train=-9.086040, val=-9.052029\n",
      "2023-11-27 16:22:18 (INFO): 9200/10000 (epoch 341):Loss: train=0.000113, val=0.000117;logMAE: train=-9.085465, val=-9.053824\n",
      "2023-11-27 16:22:22 (INFO): 9300/10000 (epoch 345):Loss: train=0.000113, val=0.000117;logMAE: train=-9.091529, val=-9.055624\n",
      "2023-11-27 16:22:27 (INFO): 9400/10000 (epoch 349):Loss: train=0.000113, val=0.000117;logMAE: train=-9.091548, val=-9.057523\n",
      "2023-11-27 16:22:32 (INFO): 9500/10000 (epoch 352):Loss: train=0.000113, val=0.000116;logMAE: train=-9.091718, val=-9.059291\n",
      "2023-11-27 16:22:37 (INFO): 9600/10000 (epoch 356):Loss: train=0.000113, val=0.000116;logMAE: train=-9.092365, val=-9.061172\n",
      "2023-11-27 16:22:41 (INFO): 9700/10000 (epoch 360):Loss: train=0.000112, val=0.000116;logMAE: train=-9.095964, val=-9.063029\n",
      "2023-11-27 16:22:46 (INFO): 9800/10000 (epoch 363):Loss: train=0.000112, val=0.000116;logMAE: train=-9.096067, val=-9.064817\n",
      "2023-11-27 16:22:51 (INFO): 9900/10000 (epoch 367):Loss: train=0.000112, val=0.000115;logMAE: train=-9.098364, val=-9.066655\n",
      "2023-11-27 16:22:55 (INFO): 10000/10000 (epoch 371):Loss: train=0.000112, val=0.000115;logMAE: train=-9.097916, val=-9.068466\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "    else:\n",
    "        step_init = 1\n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # Perform training step\n",
    "        trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "                \n",
    "            epoch = step // steps_per_epoch\n",
    "            logging.info(\n",
    "                f\"{step}/{num_steps} (epoch {epoch + 1}):\"\n",
    "                f\"Loss: train={train['metrics'].loss:.6f}, val={validation['metrics'].loss:.6f};\"\n",
    "                f\"logMAE: train={train['metrics'].mean_log_mae:.6f}, \"\n",
    "                f\"val={validation['metrics'].mean_log_mae:.6f}\"\n",
    "            )\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
